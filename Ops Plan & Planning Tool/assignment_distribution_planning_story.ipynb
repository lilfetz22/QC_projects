{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily Assignment Planning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction and Process Overview\n",
    "\n",
    "In healthcare data management, efficient allocation of medical abstractors to various facilities is crucial for maintaining accurate and timely medical records. This notebook documents the systematic approach used to manage daily abstractor assignments across multiple healthcare facilities, ensuring optimal resource utilization while meeting specific skill requirements and time constraints.\n",
    "\n",
    "### Process Context\n",
    "The daily assignment planning process involves matching qualified medical abstractors with facility requests based on several key factors:\n",
    "\n",
    "1. **Incoming Requests**\n",
    "   - Requests are received through Salesforce\n",
    "   - Each request specifies:\n",
    "     - Target facility (Hospital X, Y, Z, etc.)\n",
    "     - Required hours of work (n, m, o hours)\n",
    "     - Specific skill requirements (XX, YY, ZZ)\n",
    "\n",
    "2. **Resource Assessment**\n",
    "   - Query Salesforce database for available abstractors\n",
    "   - Filter abstractors based on required skills\n",
    "   - Evaluate abstractor availability and current workload\n",
    "   - Consider geographical and temporal constraints\n",
    "\n",
    "3. **Matching Logic**\n",
    "   - Align abstractor qualifications with facility requirements\n",
    "   - Balance workload distribution\n",
    "   - Optimize for efficiency and quality of service\n",
    "\n",
    "### Purpose of This Notebook\n",
    "This Jupyter notebook serves as both documentation and an operational tool, demonstrating the step-by-step process of:\n",
    "- Processing incoming facility requests\n",
    "- Analyzing abstractor availability and qualifications\n",
    "- Generating and validating assignments\n",
    "\n",
    "The following sections will detail each step of the process, including Python code implementations and data analysis techniques used to ensure optimal assignment distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first to import the necessary modules and create a connection to Salesforce\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from simple_salesforce import Salesforce\n",
    "# Import the module under a specific name\n",
    "import importlib\n",
    "import sf_queries_class\n",
    "importlib.reload(sf_queries_class)\n",
    "from sf_queries_class import SfQueries\n",
    "import my_sf_secrets\n",
    "import capacity_portion\n",
    "importlib.reload(capacity_portion)\n",
    "import create_capbase\n",
    "importlib.reload(create_capbase)\n",
    "my_sf_username, my_sf_password, my_sf_security_token = my_sf_secrets.get_my_sf_secrets()\n",
    "\n",
    "# queries module that I created to retrieve data from Salesforce that I was consistently using\n",
    "queries = SfQueries(\n",
    "    username=my_sf_username,\n",
    "    password=my_sf_password,\n",
    "    security_token=my_sf_security_token\n",
    ")\n",
    "from reportforce import Reportforce\n",
    "rf = Reportforce(session_id=queries.sf.session_id, instance_url=queries.sf.sf_instance)\n",
    "# get the username from the system\n",
    "username = os.getlogin()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demand Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data was gathered from Salesforce to understand what the current needs of the business are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query has been modified to give the general structure of the query, \n",
    "# but all of the actual column names removed\n",
    "sf_query = \"\"\"\n",
    "SELECT \n",
    "-- ALL_Necessary_Columns\n",
    "\n",
    "FROM \n",
    "    Table__c \n",
    "WHERE \n",
    "    Filters\n",
    "\"\"\"\n",
    "\n",
    "requests_fil = queries.convert_salesforce_data_to_df(queries.sf.query_all(query=sf_query))\n",
    "requests_fil = requests_fil.rename(columns={\n",
    "    'salesforce_name__c': 'needed_name'\n",
    "})\n",
    "\n",
    "# Calculate 'Age_Days'\n",
    "requests_fil['Created Date'] = pd.to_datetime(requests_fil['Created Date']) \n",
    "requests_fil['Age_Days'] = (datetime.now().astimezone() - requests_fil['Created Date']).dt.days\n",
    "\n",
    "# Min-Max scaling for to create a `normalized_days` column that can add a decimal value \n",
    "# to the priority total to create a composite score that can be used to differentiate between\n",
    "# requests as older requests are given higher priority when the Priority Total is the same.\n",
    "min_days = requests_fil['Age_Days'].min()\n",
    "max_days = requests_fil['Age_Days'].max() + 2\n",
    "requests_fil['normalized_days'] = (requests_fil['Age_Days'] - min_days) / (max_days - min_days)\n",
    "\n",
    "# Calculate 'composite'\n",
    "requests_fil['composite'] = requests_fil['Priority Total'] + requests_fil['normalized_days']\n",
    "\n",
    "# Create 'In Implementation' column that tells us whether the request is a backfill for a project \n",
    "# that is already running, or if it is a new request for a new project.\n",
    "requests_fil['In Implementation'] = requests_fil['Stage'].apply(lambda x: 1 if x in ['In Implementation', 'Delayed'] else 0)\n",
    "\n",
    "# but all of the actual column names removed\n",
    "requests_sel_1 = requests_fil.loc[:, ['Relevant_column_names',]].drop_duplicates()\n",
    "categories = requests_sel_1['Category'].unique()\n",
    "categories = [category for category in categories if category != 'Unecessary Category 1'] # category != 'Unecessary Category 2' and \n",
    "requests_sel = requests_sel_1.loc[requests_sel_1['Category'].isin(categories), :].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much of the work was in placing abstractors with the correct skills and capcity in the correct location, then having a template that I could easily use daily to create the assignments. This next block of code creates the template that I used to create the assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_assignment_template(requests_df):\n",
    "    \"\"\"\n",
    "    Creates a standardized template for processing assignment requests\n",
    "    \"\"\"\n",
    "    # Create a copy of the input dataframe to avoid modifications to original data\n",
    "    template_df = requests_df.copy()\n",
    "    \n",
    "    # Step 1: Process Category Classifications\n",
    "    # Replace specific categories based on business rules\n",
    "    # Comments explain what's happening without revealing specific logic\n",
    "    template_df['Category'] = np.where(\n",
    "        (template_df['Category'].str.contains('CATEGORY_A')) |\n",
    "        (template_df['Category'].isnull()),\n",
    "        template_df['Category'],\n",
    "        'Other'\n",
    "    )\n",
    "    \n",
    "    # Step 2: Additional category processing based on team assignments\n",
    "    template_df['Category'] = np.where(\n",
    "        (template_df['Team_Name'].str.contains('TEAM_A')) &\n",
    "        (template_df['Category'] == 'Other'),\n",
    "        'CATEGORY_A',\n",
    "        template_df['Category']\n",
    "    )\n",
    "    \n",
    "    # Step 3: Initialize new columns for assignment processing\n",
    "    # Set default values for required fields\n",
    "    template_df['Suggested_Resource'] = np.nan\n",
    "    template_df['Resource_ID'] = np.nan\n",
    "    template_df['Position_Type'] = 'DEFAULT_POSITION'\n",
    "    template_df['Access_Requirements'] = 'STANDARD_ACCESS'\n",
    "    template_df['Status_Update'] = np.nan\n",
    "    template_df['Hours_Update_Required'] = False\n",
    "    template_df['Updated_Hours'] = np.nan\n",
    "    template_df['Assignment_Status'] = \"\"\n",
    "    \n",
    "    # Step 4: Select and organize relevant columns for processing\n",
    "    columns_needed = [\n",
    "        'Category', 'Team_Name', 'Request_ID', \n",
    "        'Qualification_Score', 'Status_Update',\n",
    "        'Priority_Score', 'Requested_Hours',\n",
    "        'Suggested_Resource', 'Resource_ID',\n",
    "        'Assignment_Status', 'Eligibility_Flag',\n",
    "        'Special_Resource_Requirements', 'Implementation_Status',\n",
    "        'Notes', 'Assignment_Plan', 'Position_Type',\n",
    "        'Access_Requirements', 'Hours_Update_Required',\n",
    "        'Updated_Hours'\n",
    "    ]\n",
    "    \n",
    "    processed_template = template_df[columns_needed]\n",
    "    \n",
    "    # Step 5: Process special cases and adjustments\n",
    "    # Adjust hours for specific implementation cases\n",
    "    processed_template['Requested_Hours'] = np.where(\n",
    "        processed_template['Implementation_Status'] == 1,\n",
    "        processed_template['Requested_Hours'] * 1.3,\n",
    "        processed_template['Requested_Hours']\n",
    "    )\n",
    "    \n",
    "    # Step 6: Final categorization and sorting\n",
    "    processed_template['Category'] = np.where(\n",
    "        processed_template['Category'] == 'SPECIAL_CATEGORY',\n",
    "        processed_template['Alternative_Category'],\n",
    "        processed_template['Category']\n",
    "    )\n",
    "    \n",
    "    # Step 7: Sort the final template\n",
    "    processed_template = processed_template.sort_values(\n",
    "        by=['Category', 'Priority_Score', 'Team_Name'],\n",
    "        ascending=[True, False, True]\n",
    "    )\n",
    "    \n",
    "    # Step 8: Add reference IDs for system integration\n",
    "    processed_template['System_Reference_ID'] = processed_template['Request_ID'].apply(\n",
    "        lambda x: generate_reference_id(x)  # Placeholder for actual reference ID generation\n",
    "    )\n",
    "    \n",
    "    return processed_template\n",
    "\n",
    "def generate_reference_id(request_id):\n",
    "    \"\"\"\n",
    "    Placeholder function for generating system reference IDs\n",
    "    Replace with actual implementation based on your system\n",
    "    \"\"\"\n",
    "    return f\"REF_{request_id}\"\n",
    "\n",
    "requests_ready = create_assignment_template(requests_sel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capacity Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After understanding the needs of the business for the day, I needed to see what the whether any of our current resources had any availability. I used the following code to see what resources were available and had capacity to be placed on the needs above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the capbase file that determines the capacity of each abstractor in each category and saves it to a `capbase.xlsx` file\n",
    "capbase = create_capbase.create_capbase_file(queries)\n",
    "# for all the categories this creates a dictionary that holds all of the categories that are needed in order to be iterated over later\n",
    "# the writer=True parameter is used to determine whether it will be written to an excel file or not. The function is reused \n",
    "# in other notebooks to explore capacity in certain categories without running all categories, and returning it directly to the \n",
    "# notebook without writing it to an excel file\n",
    "result = capacity_portion.category_capacity(categories, capbase, writer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excel File Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I created an excel file that had the template that I made above for the Requests, and then had different tabs for all of the resources in the categories that had skills that were needed for the day's requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter(f\"Daily Tools/Assignment_Planning_{today}.xlsx\", engine='xlsxwriter')\n",
    "requests_ready.to_excel(writer, sheet_name = 'Summary', index=False)\n",
    "for cat in sorted(categories):\n",
    "    result[cat].to_excel(writer, sheet_name = cat, index = False)\n",
    "writer.close()\n",
    "# this function adjusts the column widths to fit the data so that it shows up nicely in excel\n",
    "# rather than the headers being all squashed.\n",
    "create_capbase.adjust_workbook_column_widths(f\"Daily Tools/Assignment_Planning_{today}.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diff Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An easy way to check what the changes were between the previous day and today in terms of needs and capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most recent file even if it wasn't yesterday. \n",
    "# this buffers in case I ran it the previous Friday, or \n",
    "# if I was out of the office for a few days.\n",
    "yesterday_found = False\n",
    "day = 1\n",
    "while not yesterday_found:\n",
    "    try:\n",
    "        yesterday = (datetime.today() - timedelta(days=day)).strftime('%B %d %Y').replace(\" 0\", \" \")\n",
    "        yesterday_file = f\"Daily Tools/Assignment_Planning_{yesterday}.xlsx\"\n",
    "        yesterday_xlsx = pd.ExcelFile(yesterday_file)\n",
    "        yesterday_found = True\n",
    "        yesterday_xlsx.close()\n",
    "    except:\n",
    "        day += 1\n",
    "        if day > 14:\n",
    "            print(\"no file from the last 15 days\")\n",
    "            break\n",
    "\n",
    "\n",
    "# Define file paths\n",
    "today_file = f\"Daily Tools/Assignment_Planning_{today}.xlsx\"\n",
    "\n",
    "\n",
    "def compare_summary_sheets(today_file, yesterday_file):\n",
    "    # Read the Summary sheets\n",
    "    today_summary = pd.read_excel(today_file, sheet_name='Summary')\n",
    "    yesterday_summary = pd.read_excel(yesterday_file, sheet_name='Summary')\n",
    "    yesterday_summary_fil = yesterday_summary[yesterday_summary['Request Status Update'].isnull()].copy()\n",
    "    # select just the 'Q-Centrix Team Name' and the 'Requested Hours'\n",
    "    yesterday_summary_sel = yesterday_summary_fil[['Request Name', 'Requested Hours']].rename(\n",
    "        columns={'Requested Hours': 'Requested Hours Yesterday'}).copy()\n",
    "    # add a column for yesterday\n",
    "\n",
    "    today_summary_sel = today_summary[['SPECIFIC COLUMNS']].copy()\n",
    "    summaries_joined = today_summary_sel.merge(yesterday_summary_sel, on='Request Name', how='left')\n",
    "    summaries_joined.loc[:, 'Requested Hours Yesterday'] = summaries_joined['Requested Hours Yesterday'].fillna(0)\n",
    "    summaries_joined.loc[:, 'Requested Hours Difference'] = summaries_joined['Requested Hours'] - summaries_joined['Requested Hours Yesterday']\n",
    "    differences = summaries_joined[summaries_joined['Requested Hours Difference'] > 0]\n",
    "    return differences\n",
    "\n",
    "def compare_other_sheets(today_file, yesterday_file):\n",
    "    # Get all sheet names from today's file\n",
    "    xlsx = pd.ExcelFile(today_file)\n",
    "    yesterday_xlsx = pd.ExcelFile(yesterday_file)\n",
    "    sheet_names = [sheet for sheet in xlsx.sheet_names if sheet != 'Summary']\n",
    "    \n",
    "    all_differences = {}\n",
    "\n",
    "    \n",
    "    for sheet in sheet_names:\n",
    "        # Read sheets from both files\n",
    "        dtypes = {\n",
    "            'Several_column_names': their_corresponding_dtypes,\n",
    "        }\n",
    "        today_sheet = pd.read_excel(today_file, sheet_name=f'{sheet}', dtype=dtypes)\n",
    "        if sheet not in yesterday_xlsx.sheet_names:\n",
    "            print(f\"{sheet} not in yesterday's file\")\n",
    "            today_sheet = pd.read_excel(today_file, sheet_name=f'{sheet}', dtype=dtypes)\n",
    "            today_filtered = today_sheet[today_sheet['Capticket Hours'].notnull()]\n",
    "            today_filtered_sel = today_filtered[['CBIZ_Name', 'Capticket Hours']].rename(columns={'Capticket Hours': 'Hours'}).copy()\n",
    "            if not today_filtered_sel.empty:\n",
    "                all_differences[sheet] = today_filtered_sel\n",
    "        else:\n",
    "            yesterday_sheet = pd.read_excel(yesterday_file, sheet_name=f'{sheet}', dtype=dtypes)\n",
    "            \n",
    "            # Filter rows with non-null 'Capticket Hours'\n",
    "            today_filtered = today_sheet[today_sheet['Capticket Hours'].notnull()]\n",
    "            yesterday_filtered = yesterday_sheet[yesterday_sheet['Capticket Hours'].notnull()]\n",
    "            # select just the 'CBIZ_Name' and 'Capticket Hours' columns\n",
    "            today_filtered_sel = today_filtered[['CBIZ_Name', 'Capticket Hours']].rename(columns={'Capticket Hours': 'Hours'}).copy()\n",
    "            yesterday_filtered_sel = yesterday_filtered[['CBIZ_Name', 'Capticket Hours']].rename(columns={'Capticket Hours': 'Hours'}).copy()\n",
    "            \n",
    "            # Outer join on 'CBIZ_Name'\n",
    "            merged = today_filtered_sel.merge(yesterday_filtered_sel, on='CBIZ_Name', how='left', suffixes=('_today', '_yesterday'))\n",
    "\n",
    "            # fill na with 0 in '_yesterday'\n",
    "            merged.loc[:, 'Hours_yesterday'] = merged['Hours_yesterday'].fillna(0)\n",
    "            \n",
    "            # Compare 'Capticket Hours'\n",
    "            differences = merged[merged['Hours_today'] >= merged['Hours_yesterday']]\n",
    "            \n",
    "            if not differences.empty:\n",
    "                all_differences[sheet] = differences\n",
    "    xlsx.close()\n",
    "    yesterday_xlsx.close()\n",
    "    return all_differences\n",
    "summary_differences = compare_summary_sheets(today_file, yesterday_file)\n",
    "other_sheet_differences = compare_other_sheets(today_file, yesterday_file)\n",
    "\n",
    "print(\"Differences in Summary sheet:\")\n",
    "summary_differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDifferences in other sheets:\")\n",
    "for sheet, diff in other_sheet_differences.items():\n",
    "    print(f\"\\nDifferences in {sheet}:\")\n",
    "    try:\n",
    "        print(diff[['CBIZ_Name', 'Hours_today', 'Hours_yesterday']])\n",
    "    except:\n",
    "        print(diff[['CBIZ_Name', 'Hours']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Creation table for push to SF from XL-Connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignment_decision_summary = pd.read_excel(f\"Daily Tools/Assignment_Planning_{today}.xlsx\", sheet_name = 'Summary')\n",
    "# if the entire column of 'Suggested Resource' is null this means that no decisions have been made yet \n",
    "# so throw an error message saying to input the Suggested resources into the 'Suggested Resource' column\n",
    "decisions = assignment_decision_summary[~assignment_decision_summary['Suggested Resource'].isnull()].copy()\n",
    "if len(decisions) == 0:\n",
    "    raise ValueError('Please input the Suggested resources into the \"Suggested Resource\" column')\n",
    "# check that the number of decisions is what I expected based on the plan I made today\n",
    "len(decisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_existing_assignments(proposed_assignments: pd.DataFrame, \n",
    "                                data_interface = queries) -> bool:\n",
    "    \"\"\"\n",
    "    Validates proposed assignments against existing assignments to identify conflicts\n",
    "    and necessary updates.\n",
    "    \n",
    "    Args:\n",
    "        proposed_assignments (pd.DataFrame): DataFrame containing proposed assignment data\n",
    "            Required columns:\n",
    "            - 'Resource_Name': Name of resource to be assigned\n",
    "            - 'Request_ID': Unique identifier for the request\n",
    "            - 'Team_Name': Name of team for assignment\n",
    "        data_interface: Interface for querying assignment data\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if updates to existing assignments are needed, False otherwise\n",
    "    \"\"\"\n",
    "    # Counter for assignments that need updating\n",
    "    assignments_requiring_updates = 0\n",
    "    \n",
    "    # Iterate through each proposed assignment\n",
    "    for _, assignment in proposed_assignments.iterrows():\n",
    "        # Step 1: Get resource identifier\n",
    "        try:\n",
    "            resource_id = data_interface.get_resource_id(assignment['Resource_Name'])\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching resource ID: {e}\")\n",
    "            continue\n",
    "            \n",
    "        # Step 2: Determine team identifier\n",
    "        team_id = None\n",
    "        request_id = assignment['Request_ID']\n",
    "        \n",
    "        # First try to get team ID from request if it's a valid request ID\n",
    "        if isinstance(request_id, str) and request_id.startswith(('PREFIX1', 'PREFIX2')):\n",
    "            try:\n",
    "                team_info = data_interface.get_team_info(request_id)\n",
    "                team_id = team_info.get('team_id')\n",
    "            except Exception as e:\n",
    "                print(f\"Could not retrieve team info for request: {request_id}\")\n",
    "        \n",
    "        # Fallback: Get team ID directly from team name\n",
    "        if team_id is None:\n",
    "            try:\n",
    "                team_id = data_interface.get_team_id(assignment['Team_Name'])\n",
    "            except Exception as e:\n",
    "                print(f\"Could not retrieve team ID for: {assignment['Team_Name']}\")\n",
    "                continue\n",
    "                \n",
    "        # Step 3: Check existing assignments\n",
    "        if team_id:\n",
    "            try:\n",
    "                # Get current assignments for the resource\n",
    "                current_assignments = data_interface.get_resource_assignments(\n",
    "                    resource_id=resource_id\n",
    "                )\n",
    "                \n",
    "                # Check if resource is already assigned to this team\n",
    "                if assignment['Team_Name'] in current_assignments['Team_Name'].values:\n",
    "                    assignments_requiring_updates += 1\n",
    "                    print(f\"Existing assignment found for {assignment['Resource_Name']} \"\n",
    "                          f\"on {assignment['Team_Name']}\")\n",
    "                    \n",
    "                    # Get and display other team members for reference\n",
    "                    team_assignments = data_interface.get_resource_assignments(team_id=team_id)\n",
    "                    active_team_members = team_assignments[\n",
    "                        team_assignments['Position'] == 'STANDARD_POSITION'\n",
    "                    ]['Resource_Name']\n",
    "                    print(\"Current team assignments:\")\n",
    "                    print(active_team_members)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error checking assignments: {e}\")\n",
    "                continue\n",
    "    \n",
    "    # Return whether any updates are needed\n",
    "    return assignments_requiring_updates > 0\n",
    "\n",
    "def check_assignment_updates(decisions_df: pd.DataFrame) -> bool:\n",
    "    \"\"\"\n",
    "    Wrapper function to check if assignments need updating\n",
    "    \n",
    "    Args:\n",
    "        decisions_df: DataFrame containing assignment decisions\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if updates are needed, False otherwise\n",
    "    \"\"\"\n",
    "    return validate_existing_assignments(decisions_df)\n",
    "\n",
    "assignment_update_needed = check_assignment_updates(decisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excel Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After checking if an assignment exists I would reformat the dataframe to fit the format that I needed to push to Salesforce. Then I would get this data and put it along with another dataframe that contained the capacity ticket information and save them both as separate sheets in a new Excel Workbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code removed for privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then updated the requests from the decisions dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisions_no_na_requests = assignment_decision_summary.loc[assignment_decision_summary['Request Status Update'].notna()]\n",
    "decisions_requests = decisions_no_na_requests.loc[decisions_no_na_requests['Request Status Update'].str.contains('^(PREFIX)*'), \n",
    "                                                    ['SPECIFIC COLUMS']]\n",
    "# if the Request name value contains 'see' then remove the row\n",
    "decisions_requests = decisions_requests.drop_duplicates()\n",
    "decisions_requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(decisions_requests)):\n",
    "    request_info = queries.get_sr_from_request_name(decisions_requests['Request Name'].iloc[i])\n",
    "    if decisions_requests['Request Hours Update?'].iloc[i] == 'TRUE':\n",
    "        request_info['SOW_Hours__c'].values[0] = decisions_requests['Update Request Hours To'].iloc[i]\n",
    "        if pd.isna(decisions_requests['Notes'].iloc[i]):\n",
    "            print(f\"Skipping Notes Update for {decisions_requests['Request Name'].iloc[i]} on {decisions_requests['Team Name'].iloc[i]}\")\n",
    "        else:\n",
    "            request_info.loc[:, 'Notes__c'] = np.where((request_info['Notes__c'].values[0] != decisions_requests['Notes'].iloc[i]), \n",
    "                                                    (request_info['Notes__c'].values[0] + decisions_requests['Notes'].iloc[i]),\n",
    "                                                    request_info['Notes__c'])\n",
    "    queries.sf.Staffing_Request__c.update(request_info.Id.values[0], {'SOW_Hours__c': request_info.SOW_Hours__c.values[0], \n",
    "                                                    'Status__c': decisions_requests['Request Status Update'].iloc[i],\n",
    "                                                    \"Notes__c\": f\"{request_info.Notes__c.values[0]}\"})\n",
    "    print(f\"Updated {decisions_requests['Request Name'].iloc[i]} on {decisions_requests['Team Name'].iloc[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# History of Assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignment_history_df = pd.read_excel(f\"assignment_creation_history.xlsx\", sheet_name='Automated_Assignment_Creation')\n",
    "old_assignment_history = pd.read_excel(f\"assignment_creation_history.xlsx\", sheet_name='Assignment Creation (3)')\n",
    "decisions.loc[:, 'Date Created'] = datetime.now().date()\n",
    "assignment_history_df = pd.concat([assignment_history_df, decisions]).drop_duplicates()\n",
    "writer = pd.ExcelWriter(\"assignment_creation_history.xlsx\", engine='xlsxwriter')\n",
    "assignment_history_df.to_excel(writer, sheet_name = 'Automated_Assignment_Creation', index=False)\n",
    "old_assignment_history.to_excel(writer, sheet_name = 'Assignment Creation (3)', index=False)\n",
    "writer.close()\n",
    "create_capbase.adjust_workbook_column_widths(f\"assignment_creation_history.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
