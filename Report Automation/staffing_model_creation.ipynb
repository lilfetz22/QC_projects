{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from simple_salesforce import Salesforce\n",
    "from sf_queries_class import SfQueries\n",
    "import my_sf_secrets\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import calendar\n",
    "import create_capbase\n",
    "from reportforce import Reportforce\n",
    "from lifelines import KaplanMeierFitter\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "username = os.getlogin()\n",
    "\n",
    "path_to_planning_teams_folder = f\"C:/Users/{username}/Q-Centrix, LLC/Senior Manager Projects - Consolidated Planning Tool/\" \n",
    "path_to_ops_teams_folder = f\"C:/Users/{username}/Q-Centrix, LLC/Senior Manager Projects - Ops Plan/\"\n",
    "path_to_staffing_models_folder = f\"C:/Users/{username}/Q-Centrix, LLC/WFM QCI Centralization - Staffing Models/\"\n",
    "\n",
    "my_sf_username, my_sf_password, my_sf_security_token = my_sf_secrets.get_my_sf_secrets()\n",
    "queries = SfQueries(\n",
    "    username=my_sf_username,\n",
    "    password=my_sf_password,\n",
    "    security_token=my_sf_security_token\n",
    ")\n",
    "\n",
    "rf = Reportforce(session_id=queries.sf.session_id, instance_url=queries.sf.sf_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_rate_csv = False\n",
    "today = (datetime.today() - relativedelta(days=0)).strftime('%B %d %Y').replace(\" 0\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_day_of_month(date_input, year=datetime.now().year):\n",
    "    \"\"\"\n",
    "    Returns the last day of the month for the given date input.\n",
    "\n",
    "    Parameters:\n",
    "    date_input (str): A string representing a month name, month number, or a date.\n",
    "\n",
    "    Returns:\n",
    "    str: The last day of the month in YYYY-MM-DD format.\n",
    "    \"\"\"\n",
    "    # if the input is a number, convert it to a string\n",
    "    if (isinstance(date_input, int)):\n",
    "        # convert it to a string\n",
    "        date_input = str(date_input)\n",
    "    try:\n",
    "        # Try to parse the input as a date\n",
    "        parsed_date = datetime.strptime(date_input, '%Y-%m-%d')\n",
    "    except ValueError:\n",
    "        # If parsing fails, try to interpret the input as a month name or number\n",
    "        try:\n",
    "            # Attempt to parse as a full month name, defaulting to the current year\n",
    "            parsed_date = datetime.strptime(date_input + ' ' + str(year), '%B %Y')\n",
    "        except ValueError:\n",
    "            try:\n",
    "                # Attempt to parse as an abbreviated month name, defaulting to the current year\n",
    "                parsed_date = datetime.strptime(date_input + ' ' + str(year), '%b %Y')\n",
    "            except ValueError:\n",
    "                # If still failing, treat the input as a month number\n",
    "                month_number = int(date_input)\n",
    "                if 1 <= month_number <= 12:\n",
    "                    # Default to the current year if no year is specified\n",
    "                    parsed_date = datetime(year, month_number, 1)\n",
    "                else:\n",
    "                    raise ValueError(f\"Invalid month number: {date_input}\")\n",
    "\n",
    "    # Calculate the last day of the month\n",
    "    _, last_day = calendar.monthrange(parsed_date.year, parsed_date.month)\n",
    "    # if first_day == 0:\n",
    "    #     first_day = 1\n",
    "    last_day_str = datetime(parsed_date.year, parsed_date.month, last_day)#.strftime('%B %d %Y').replace(\" 0\", \" \")\n",
    "    # first_day_str = datetime(parsed_date.year, parsed_date.month, 1)\n",
    "    return last_day_str\n",
    "\n",
    "# create the target column which is any negative Need rounded to the nearest .5 or whole number\n",
    "def custom_round(value):\n",
    "    \"\"\"\n",
    "    Rounds a value to the nearest 0.5 or whole number based on its proximity to 0.5 or 1.0.\n",
    "    \n",
    "    Parameters:\n",
    "    - value: float, the value to round\n",
    "    \n",
    "    Returns:\n",
    "    - Rounded value as a float\n",
    "    \"\"\"\n",
    "    int_value = int(value)\n",
    "    abs_value = abs(value - int_value)\n",
    "    if abs_value >= 0.5 and abs_value < 1.0:\n",
    "        return np.round(value)\n",
    "    elif value < -0.1:\n",
    "        return int_value - 0.5\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def final_column_adj_fx(df):\n",
    "    # fill na with 0 in 'Pipe Raw, Pipeline Adjusted, Attrit, [n]m, 30-days Attrit, [n]m' columns\n",
    "    df.loc[:, 'Pipe Raw'] = df['Pipe Raw'].fillna(0)\n",
    "    df.loc[:, 'Pipeline Adjusted'] = df['Pipeline Adjusted'].fillna(0)\n",
    "    df.loc[:, 'Attrit, [n]m'] = df['Attrit, [n]m'].fillna(0)\n",
    "    df.loc[:, '30-days Attrit, [n]m'] = df['30-days Attrit, [n]m'].fillna(0)\n",
    "    df.loc[:, 'Demand_FTE'] = df['Demand_FTE'].fillna(0)\n",
    "    df.loc[:, 'Capacity_FTE'] = df['Capacity_FTE'].fillna(0)\n",
    "\n",
    "    # make columns negative\n",
    "    df.loc[:, 'Pipe Raw'] = df['Pipe Raw'] * -1\n",
    "    df.loc[:, 'Pipeline Adjusted'] = df['Pipeline Adjusted'] * -1\n",
    "    df.loc[:, 'Demand_FTE'] = df['Demand_FTE'] * -1\n",
    "    df.loc[:, 'Attrit, [n]m'] = df['Attrit, [n]m'] * -1\n",
    "    df.loc[:, '30-days Attrit, [n]m'] = df['30-days Attrit, [n]m'] * -1\n",
    "\n",
    "    df.loc[:, 'Need'] = df['Demand_FTE'] + \\\n",
    "    df['Capacity_FTE'] + df['Pipeline Adjusted'] + \\\n",
    "        df['Attrit, [n]m']\n",
    "\n",
    "    df.loc[:, 'Target'] = df['Need'].apply(custom_round) * -1\n",
    "    df.loc[:, 'Target'] = np.where(\n",
    "        df['Target'] < 0, 0, df['Target'])\n",
    "\n",
    "    # round all columns to 3 decimal places except the Target column\n",
    "    df = df.round({'Pipe Raw': 3, 'Win Rate': 3, 'Pipeline Adjusted': 3, 'Need': 3,\n",
    "                   'Attrit, [n]m': 3, '30-days Attrit, [n]m': 3, 'age_closure_prob': 3,\n",
    "                   'Predicted Win Probability': 3})\n",
    "    return df\n",
    "def prod_with_nan_as_zero(x):\n",
    "    return np.prod(np.where(np.isnan(x), 0, x))\n",
    "\n",
    "def create_monthly_snapshots(opportunities_df, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Create monthly snapshots of contracted and implemented hours by category.\n",
    "    \n",
    "    Parameters:\n",
    "    opportunities_df: DataFrame with columns ['Category', 'Start_Date', 'End_Date', \n",
    "                     'Contracted_Hours', 'Implemented_Weekly_Hours', 'Stage']\n",
    "    start_date: datetime object for the start of the analysis period\n",
    "    end_date: datetime object for the end of the analysis period\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a date range for all months in the analysis period\n",
    "    months = pd.date_range(start=start_date, end=end_date, freq='MS')\n",
    "    \n",
    "    snapshots = []\n",
    "    \n",
    "    for month_start in months:\n",
    "        month_end = month_start + pd.offsets.MonthEnd(0)\n",
    "        \n",
    "        # Filter opportunities that are active in the current month\n",
    "        active_mask = (\n",
    "            (opportunities_df['Opportunity Start Date'] < month_start) & \n",
    "            (opportunities_df['Close Date'] >= month_end)\n",
    "        )\n",
    "        active_opportunities = opportunities_df[active_mask]\n",
    "        \n",
    "        # Calculate contracted hours by category\n",
    "        contracted_hours = (\n",
    "            active_opportunities\n",
    "            .groupby('Category')['Contracted Weekly Hours']\n",
    "            .sum()\n",
    "            .reset_index()\n",
    "        )\n",
    "        \n",
    "        # Calculate implemented hours only for opportunities with 'Contract Signed (Implementing)' stage\n",
    "        implementing_opportunities = opportunities_df[ \n",
    "            ((opportunities_df['Stage'] == 'Contract Signed (Implementing)') | (opportunities_df['Stage'] == 'Customer Invoiced')) &\n",
    "            (\n",
    "            (opportunities_df['Close Date'] >= month_start) &\n",
    "            (opportunities_df['Close Date'] <= month_end)\n",
    "        )\n",
    "        ].copy()\n",
    "        \n",
    "        implemented_hours = (\n",
    "            implementing_opportunities\n",
    "            .groupby('Category')['Implemented Weekly Hours']\n",
    "            .sum()\n",
    "            .reset_index()\n",
    "        )\n",
    "        \n",
    "        # Merge the results\n",
    "        month_snapshot = pd.merge(\n",
    "            contracted_hours,\n",
    "            implemented_hours,\n",
    "            on='Category',\n",
    "            how='outer'\n",
    "        ).fillna(0)\n",
    "        \n",
    "        # Add month information\n",
    "        month_snapshot['Month'] = month_start.strftime('%Y-%m')\n",
    "        \n",
    "        snapshots.append(month_snapshot)\n",
    "    \n",
    "    # Combine all monthly snapshots\n",
    "    final_df = pd.concat(snapshots, ignore_index=True)\n",
    "    \n",
    "    # Reorder columns\n",
    "    final_df = final_df[['Month', 'Category', 'Contracted Weekly Hours', 'Implemented Weekly Hours']]\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Current Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You need OFPH\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14451"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if 'ofph' not in globals():\n",
    "    print(\"You need OFPH\")\n",
    "    ofph = rf.get_report(\"00Oan0000038NlhEAE\", id_column='Opportunity ID') #'Opp+Cat'\n",
    "len(ofph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ofph.loc[:, 'Days to Close'] = (ofph['Close Date'] - datetime.now()).dt.days\n",
    "ofph_fil_1 = ofph[(ofph['Services ACV above $250k'] == 'false') & \n",
    "                         (ofph['Probability (%)'] < 90) & \n",
    "                         (ofph['Probability (%)'] >= 5) & \n",
    "                         (~ofph['Stage'].isin(['Closed Lost', 'Contract Signed (Implementing)'])) & \n",
    "                         (ofph['Days to Close'] <= 90) &\n",
    "                         (ofph['Opportunity Owner'] != 'Michelle Galvan') &\n",
    "                         (ofph['Exclude from Resource Requests'] == 'false')].copy().reset_index()\n",
    "ofph_fil = ofph_fil_1[ofph_fil_1['Category'] != 'Technology'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1243 entries, 0 to 1331\n",
      "Data columns (total 27 columns):\n",
      " #   Column                                 Non-Null Count  Dtype         \n",
      "---  ------                                 --------------  -----         \n",
      " 0   Opportunity Name                       1243 non-null   object        \n",
      " 1   Category                               1243 non-null   object        \n",
      " 2   Opportunity Owner                      1243 non-null   object        \n",
      " 3   Type                                   1243 non-null   object        \n",
      " 4   Additional Information for PM Handoff  1243 non-null   object        \n",
      " 5   Probability (%)                        1243 non-null   int64         \n",
      " 6   Case Type                              1243 non-null   object        \n",
      " 7   Contracted Weekly Hours                1243 non-null   float64       \n",
      " 8   Monthly Quantity                       1243 non-null   float64       \n",
      " 9   Backlog                                1243 non-null   object        \n",
      " 10  Contracted Opportunity ACV             1243 non-null   float64       \n",
      " 11  Account Name                           1243 non-null   object        \n",
      " 12  Hospital Count                         1243 non-null   float64       \n",
      " 13  Group Name                             1243 non-null   object        \n",
      " 14  Account                                1243 non-null   object        \n",
      " 15  Business Leader                        1243 non-null   object        \n",
      " 16  Implemented Weekly Hours               1243 non-null   float64       \n",
      " 17  Implemented Quantity                   1243 non-null   float64       \n",
      " 18  Close Date                             1243 non-null   datetime64[ns]\n",
      " 19  Stage                                  1243 non-null   object        \n",
      " 20  Forecast Type                          1243 non-null   object        \n",
      " 21  Opportunity ID                         1243 non-null   object        \n",
      " 22  Services ACV above $250k               1243 non-null   object        \n",
      " 23  Age                                    1243 non-null   int64         \n",
      " 24  Exclude from Resource Requests         1243 non-null   object        \n",
      " 25  Opp+Cat                                1243 non-null   object        \n",
      " 26  Days to Close                          1243 non-null   int64         \n",
      "dtypes: datetime64[ns](1), float64(6), int64(3), object(17)\n",
      "memory usage: 271.9+ KB\n"
     ]
    }
   ],
   "source": [
    "ofph_fil.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ofph_fil.loc[:, 'Category'] = np.where((ofph_fil['Category'] == 'Cancer') & \n",
    "                                                ((ofph_fil['Case Type'].str.contains('Follow up')) |\n",
    "                    (ofph_fil['Case Type'].str.contains('Case Finding'))), 'Cancer - CDSS', \n",
    "                    np.where((ofph_fil['Category'] == 'Cancer') & \n",
    "                                                (~(ofph_fil['Case Type'].str.contains('Follow up')) &\n",
    "                    ~(ofph_fil['Case Type'].str.contains('Case Finding'))), 'Cancer - Abstractor', ofph_fil['Category']))\n",
    "ofph_fil.loc[:, 'FTE'] = ofph_fil['Contracted Weekly Hours'] / 40\n",
    "ofph_gr = ofph_fil.loc[:, ['Category', 'FTE']].groupby('Category').sum()[['FTE']].reset_index().rename(\n",
    "    columns={'FTE': 'Pipe Raw', 'Category': 'Product Category'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supply Demand Summary Pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = ['ACD CP', 'ACD HF', 'Advisory - Spark', 'Advisory - Validation', 'AFib Ablation', 'AJRR', \n",
    "        'Burn-ABA', 'Cancer - Abstractor', 'Cancer - CDSS', 'Chest Pain MI', 'CM', 'CMQCC', 'Concurrent', 'CPQCC', \n",
    "        'ELSO Registry', 'Other', 'GIQuIC', 'GWTG AFIB', 'GWTG Cardiogenic Shock', 'GWTG HF', \n",
    "        'GWTG Resuscitation', 'GWTG Stroke', 'GWTG-CAD', 'ICD', 'Infection Prevention', 'Intermacs', \n",
    "        'LAAO', 'MBSAQIP', 'MH', 'Mortality Review', 'NSQIP', 'NYS Sepsis', 'PCI', 'Registry', \n",
    "        'Registry Capture', 'Other', 'STEMI', 'STS-ACS', 'STS-CHS', 'STS-GTS', 'Trauma', 'TVT', 'VON', 'VQI']\n",
    "cats_set = list(set(cats + list(ofph_fil.Category.unique())))\n",
    "if 'Cancer' in cats_set:\n",
    "        cats_set.pop(cats_set.index('Cancer'))\n",
    "cats_set = sorted(cats_set)\n",
    "all_product_categories_sel = pd.DataFrame({'Product Category': cats_set})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Category</th>\n",
       "      <th>Demand_FTE</th>\n",
       "      <th>Capacity_FTE</th>\n",
       "      <th>Pipe Raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACD CP</td>\n",
       "      <td>0.30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACD HF</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.28</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFib Ablation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.241725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AJRR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.081375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Advisory - Spark</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Product Category  Demand_FTE  Capacity_FTE  Pipe Raw\n",
       "0            ACD CP        0.30           NaN       NaN\n",
       "1            ACD HF        0.03          0.28       NaN\n",
       "2     AFib Ablation         NaN           NaN  0.241725\n",
       "3              AJRR         NaN          0.02  0.081375\n",
       "4  Advisory - Spark         NaN           NaN  0.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files = glob.glob(f\"{path_to_planning_teams_folder}Supply Demand Summary/supply_demand_analysis_*.xlsx\")\n",
    "supply_demand_analysis_path = max(all_files, key=os.path.getctime)\n",
    "supply_demand_analysis_file = pd.read_excel(supply_demand_analysis_path, sheet_name=\"Summary\", skiprows=1)\n",
    "# remove row 0 and rename the `Unnamed: 0` column to `Product Category`\n",
    "supply_demand_analysis_file = supply_demand_analysis_file.iloc[1:].rename(columns={\"Unnamed: 0\": \"Product Category\"})\n",
    "supply_demand = supply_demand_analysis_file.loc[:, [\"Product Category\", \"Requested FTE\", \"FTE capacity\"]]\n",
    "# # if the category contains \"cancer\" then replace with \"Cancer\"\n",
    "# supply_demand[\"Product Category\"] = supply_demand[\"Product Category\"].apply(lambda x: \"Cancer\" if \"Cancer\" in x else x)\n",
    "# # combine the 2 rows in supply_demand_analysis_file that contain \"Cancer\"\n",
    "# supply_demand = supply_demand.groupby(\"Product Category\").sum().reset_index()\n",
    "# join with all_product_categories\n",
    "supply_demand_all_products = pd.merge(all_product_categories_sel, supply_demand, on=\"Product Category\", \n",
    "                                      how=\"left\")\n",
    "# rename the Requested FTE and FTE capacity columns to Demand_FTE and Capacity_FTE\n",
    "supply_demand_all_products = supply_demand_all_products.rename(columns={\"Requested FTE\": \"Demand_FTE\", \n",
    "                                                                        \"FTE capacity\": \"Capacity_FTE\"})\n",
    "# merge with ops_plan_ofph_gr\n",
    "supply_demand_all_products_pipe_raw = pd.merge(supply_demand_all_products, ofph_gr, on=\"Product Category\", how=\"left\")\n",
    "supply_demand_all_products_pipe_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Win Rate Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Win Rate CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if win_rate_csv:\n",
    "    win_rate_modifier = pd.read_csv('win_rate_modifier.csv')\n",
    "    win_rate_modifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if win_rate_csv:\n",
    "    supply_demand_all_products_pipe_raw_win_merged = supply_demand_all_products_pipe_raw.merge(win_rate_modifier, on='Product Category', how='left')\\\n",
    "        .rename(columns={'Pipeline Modifier': 'Win Rate'})\n",
    "    # fill na with 0.3\n",
    "    supply_demand_all_products_pipe_raw_win_merged['Win Rate'] = supply_demand_all_products_pipe_raw_win_merged['Win Rate'].fillna(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Win Rate Calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6684\n"
     ]
    }
   ],
   "source": [
    "if not win_rate_csv:\n",
    "    ofph_monthly = ofph[(ofph['Opportunity Owner'] != 'Michelle Galvan') &\n",
    "                        (ofph['Type'].isin(['New Client', 'Existing Client - New Service Line', 'Existing Client - Service Line Expansion'])) &\n",
    "                        (ofph['Exclude from Resource Requests'] == 'false')].copy()\n",
    "    ofph_monthly_idx_reset = ofph_monthly.reset_index()\n",
    "    # filter out any Technology category\n",
    "    ofph_monthly_idx_reset = ofph_monthly_idx_reset[~ofph_monthly_idx_reset['Category'].str.contains('Technology')]\n",
    "    ofph_monthly_idx_reset.loc[:, 'Category'] = np.where((ofph_monthly_idx_reset['Category'] == 'Cancer') & \n",
    "                                                ((ofph_monthly_idx_reset['Case Type'].str.contains('Follow up')) |\n",
    "                    (ofph_monthly_idx_reset['Case Type'].str.contains('Case Finding'))), 'Cancer - CDSS', \n",
    "                    np.where((ofph_monthly_idx_reset['Category'] == 'Cancer') & \n",
    "                                                (~(ofph_monthly_idx_reset['Case Type'].str.contains('Follow up')) &\n",
    "                    ~(ofph_monthly_idx_reset['Case Type'].str.contains('Case Finding'))), 'Cancer - Abstractor', ofph_monthly_idx_reset['Category']))\n",
    "    # groupby Opportunity ID and get the first value from 'Age', 'Backlog', and 'Hospital Count'\n",
    "    abhc = ofph_monthly_idx_reset.groupby(['Opportunity ID', 'Category']).first()[['Opportunity Start Date', 'Close Date', 'Stage']]\n",
    "    # group by Opportunity ID and calculate the sum for all the other columns\n",
    "    rest = ofph_monthly_idx_reset.groupby(['Opportunity ID', 'Category'])[['Contracted Weekly Hours', 'Implemented Weekly Hours']].sum()\n",
    "    # merge the 2 back together\n",
    "    ofph_monthly_idx_reset_merged = pd.merge(abhc, rest, left_index=True, right_index=True)\n",
    "    ofph_monthly_ready = ofph_monthly_idx_reset_merged.reset_index()\n",
    "    ofph_snapshots = create_monthly_snapshots(ofph_monthly_ready, '8/1/2022', datetime.now())\n",
    "    ofph_snapshots.loc[:, 'perc_realized'] = ofph_snapshots['Implemented Weekly Hours'] / ofph_snapshots['Contracted Weekly Hours'] \n",
    "    cat = ofph_snapshots.loc[(ofph_snapshots['Month'].year == datetime.now().year), ['Category', 'Contracted Weekly Hours', 'Implemented Weekly Hours']].groupby('Category').mean()\n",
    "    cat.loc[:, 'perc_realized'] = cat['Implemented Weekly Hours'] / cat['Contracted Weekly Hours']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implemented Vs Contracted Hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Category  Predicted Win Probability  \\\n",
      "0            AFib Ablation                   0.218357   \n",
      "1                     AJRR                   0.003175   \n",
      "2         Advisory - Spark                   0.130506   \n",
      "3    Advisory - Validation                   0.095330   \n",
      "4  American Spine Registry                   0.004267   \n",
      "\n",
      "   Contracted Weekly Hours  Implemented Weekly Hours   diff  diff_perc  \n",
      "0                      NaN                       NaN    NaN        NaN  \n",
      "1                      NaN                       NaN    NaN        NaN  \n",
      "2                    0.000                      0.00  0.000        NaN  \n",
      "3                    2.325                      2.35 -0.025    -0.0108  \n",
      "4                    6.977                      6.99 -0.013    -0.0019  \n"
     ]
    }
   ],
   "source": [
    "if not win_rate_csv:\n",
    "    con_imp_1 = ofph_fil_only_closed.reset_index()\n",
    "    con_imp_1.loc[:, 'Category'] = np.where((con_imp_1['Category'] == 'Cancer') & \n",
    "                                                ((con_imp_1['Case Type'].str.contains('Follow up')) |\n",
    "                    (con_imp_1['Case Type'].str.contains('Case Finding'))), 'Cancer - CDSS', \n",
    "                    np.where((con_imp_1['Category'] == 'Cancer') & \n",
    "                                                (~(con_imp_1['Case Type'].str.contains('Follow up')) &\n",
    "                    ~(con_imp_1['Case Type'].str.contains('Case Finding'))), 'Cancer - Abstractor', con_imp_1['Category']))\n",
    "    \n",
    "    \n",
    "    con_imp = con_imp_1.loc[(con_imp_1['Probability (%)'] > 0), \n",
    "                                        ['Category', 'Contracted Weekly Hours', 'Implemented Weekly Hours']]\\\n",
    "        .groupby('Category').sum()[['Contracted Weekly Hours', 'Implemented Weekly Hours']]\n",
    "    con_imp.loc[:, 'diff'] = con_imp['Contracted Weekly Hours'] - con_imp['Implemented Weekly Hours']\n",
    "    con_imp.loc[:, 'diff_perc'] = round(con_imp['diff'] / con_imp['Contracted Weekly Hours'], 4)\n",
    "    con_imp.loc[:, 'diff'] = round(con_imp.loc[:, 'diff'], 4)\n",
    "    # merge with the cats_w_winning_probabilities_grouped_cat dataframe on 'Product Category'\n",
    "    win_probs_hours_conversion = cats_w_winning_probabilities_grouped_cat.merge(con_imp, on='Category', how='left')\n",
    "    print(win_probs_hours_conversion.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Opportunity ID              Category  Age              Stage  Backlog  \\\n",
      "0  0065b00000xQes9  Infection Prevention  126        Closed Lost        0   \n",
      "1  0065b00000xQqCj           GWTG Stroke  132  Customer Invoiced        0   \n",
      "2  0065b00000xQuCg                   VQI   62        Closed Lost        1   \n",
      "3  0065b00000xRmkG                Trauma  217        Closed Lost        0   \n",
      "4  0065b00000xRpLn                   VQI   37  Customer Invoiced        0   \n",
      "\n",
      "   Hospital Count  Contracted Opportunity ACV Close Date  \\\n",
      "0             1.0                    82500.00 2022-12-06   \n",
      "1             1.0                    42906.65 2022-12-12   \n",
      "2             1.0                    18126.90 2022-10-03   \n",
      "3             1.0                    44956.56 2023-03-08   \n",
      "4             1.0                     6932.19 2022-09-09   \n",
      "\n",
      "                                                Type  Contracted Weekly Hours  \\\n",
      "0                 Existing Client - New Service Line                    0.004   \n",
      "1  Existing Client - New Service LineExisting Cli...                    4.205   \n",
      "2  Existing Client - Service Line ExpansionExisti...                    3.454   \n",
      "3  Existing Client - New Service LineExisting Cli...                    5.426   \n",
      "4  Existing Client - Service Line ExpansionExisti...                    3.209   \n",
      "\n",
      "   event  \n",
      "0      1  \n",
      "1      0  \n",
      "2      1  \n",
      "3      1  \n",
      "4      0  \n"
     ]
    }
   ],
   "source": [
    "if not win_rate_csv:\n",
    "  important_features_closing_age = ['Opportunity ID', ## String\n",
    "                        'Age', # Int\n",
    "                        'Type', # String\n",
    "                        'Category', # String\n",
    "                        'Contracted Weekly Hours', # Float\n",
    "                        'Backlog', # Bool\n",
    "                        'Contracted Opportunity ACV', # Float\n",
    "                        'Hospital Count', # Int\n",
    "                        'Close Date',\n",
    "                        'Stage'\n",
    "                      #   'Implemented Weekly Hours', # Float choosing Contracted for classification\n",
    "                        ]\n",
    "  cats_gr25 = ['CM',\n",
    "  'GWTG Stroke',\n",
    "  'Cancer - CDSS',\n",
    "  'Cancer - Abstractor',\n",
    "  'PCI',\n",
    "  'Infection Prevention',\n",
    "  'Trauma',\n",
    "  'TVT',\n",
    "  'STS-ACS',\n",
    "  'LAAO',\n",
    "  'VQI',\n",
    "  'Chest Pain MI',\n",
    "  'GWTG HF',\n",
    "  'MBSAQIP',\n",
    "  'GWTG-CAD',\n",
    "  'NSQIP',\n",
    "  'ICD',\n",
    "  'STS-GTS']\n",
    "  ofph_fil_closing_age = ofph[(ofph['Opportunity Owner'] != 'Michelle Galvan') &\n",
    "                  (ofph['Type'].isin(['New Client', 'Existing Client - New Service Line', 'Existing Client - Service Line Expansion'])) &\n",
    "                  (ofph['Exclude from Resource Requests'] == 'false')].copy()\n",
    "  ofph_index_reset = ofph_fil_closing_age.reset_index()\n",
    "  ofph_index_reset.loc[:, 'Category'] = np.where((ofph_index_reset['Category'] == 'Cancer') & \n",
    "                                                ((ofph_index_reset['Case Type'].str.contains('Follow up')) |\n",
    "                    (ofph_index_reset['Case Type'].str.contains('Case Finding'))), 'Cancer - CDSS', \n",
    "                    np.where((ofph_index_reset['Category'] == 'Cancer') & \n",
    "                                                (~(ofph_index_reset['Case Type'].str.contains('Follow up')) &\n",
    "                    ~(ofph_index_reset['Case Type'].str.contains('Case Finding'))), 'Cancer - Abstractor', ofph_index_reset['Category']))\n",
    "  ofph_index_reset_select_cats = ofph_index_reset[ofph_index_reset['Category'].isin(cats_gr25)].copy()\n",
    "  # convert the Backlog string column to a boolean by changing the true values to 1 and the false values to 0\n",
    "  ofph_index_reset_select_cats['Backlog'] = ofph_index_reset_select_cats['Backlog'].str.replace('true', '1').str.replace('false', '0').astype(int)\n",
    "  ofph_index_reset_sel = ofph_index_reset_select_cats[important_features_closing_age]\n",
    "\n",
    "  # groupby Opportunity ID and get the first value from 'Age', 'Backlog', and 'Hospital Count'\n",
    "  abhc = ofph_index_reset_sel.groupby(['Opportunity ID', 'Category']).first()[['Age', 'Stage', 'Backlog', 'Hospital Count', 'Contracted Opportunity ACV', 'Close Date']]\n",
    "  # group by Opportunity ID and calculate the sum for all the other columns\n",
    "  rest = ofph_index_reset_sel.drop(['Age', 'Stage', 'Backlog', 'Hospital Count', \n",
    "                                    'Contracted Opportunity ACV', 'Close Date'], axis=1).groupby(['Opportunity ID', 'Category']).sum()\n",
    "  # merge the 2 back together\n",
    "  ofph_index_reset_sel_merged = pd.merge(abhc, rest, left_index=True, right_index=True)\n",
    "  ofph_index_reset_sel_merged.loc[:, 'event'] = ((ofph_index_reset_sel_merged['Stage'] == 'Closed Lost') |\n",
    "                                                (ofph_index_reset_sel_merged['Stage'] == 'Contract Signed (Implementing)')).astype(int)\n",
    "  merged_ready = ofph_index_reset_sel_merged.reset_index()\n",
    "  print(merged_ready.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Category  age_closure_prob\n",
      "0                   CM          0.300527\n",
      "1  Cancer - Abstractor          0.431518\n",
      "2        Cancer - CDSS          0.411997\n",
      "3        Chest Pain MI          0.271109\n",
      "4              GWTG HF          0.339901\n"
     ]
    }
   ],
   "source": [
    "if not win_rate_csv:\n",
    "    kmf = KaplanMeierFitter()\n",
    "    # Assuming you have a DataFrame 'opportunities' with columns:\n",
    "    # 'Age': time since opportunity creation\n",
    "    # 'Closed': 1 if closed, 0 if still open\n",
    "    # 'ClosureTime': time to closure for closed opportunities, current age for open ones\n",
    "\n",
    "    # Fit KaplanMeierFitter on closed opportunities\n",
    "    closed_opps = merged_ready[merged_ready['event'] == 1].copy()\n",
    "    kmf.fit(durations=closed_opps['Age'], event_observed=closed_opps['event'])\n",
    "\n",
    "    # Predict for all opportunities\n",
    "    all_times = merged_ready['Age']\n",
    "    survival_probabilities = kmf.predict(all_times)\n",
    "\n",
    "    # Convert survival probabilities to closure probabilities\n",
    "    closure_probabilities = 1 - survival_probabilities\n",
    "    closure_probabilities = closure_probabilities.reset_index().rename(columns={'index': 'Duration',\n",
    "                                                                                'KM_estimate': 'age_closure_prob'})\n",
    "\n",
    "\n",
    "    # Add probabilities to the original DataFrame\n",
    "    merged_ready_w_KM_estimate = pd.merge(merged_ready, closure_probabilities, left_index=True, right_index=True)\n",
    "\n",
    "    # For open opportunities, you can now access their closure probabilities\n",
    "    open_opps = merged_ready_w_KM_estimate[merged_ready_w_KM_estimate['event'] == 0].copy()\n",
    "    open_opps_with_probs = open_opps[['Category', 'age_closure_prob']].groupby('Category').mean().reset_index()\n",
    "\n",
    "    print(open_opps_with_probs.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Category  Predicted Win Probability  \\\n",
      "0            AFib Ablation                   0.218357   \n",
      "1                     AJRR                   0.003175   \n",
      "2         Advisory - Spark                   0.130506   \n",
      "3    Advisory - Validation                   0.095330   \n",
      "4  American Spine Registry                   0.004267   \n",
      "\n",
      "   Contracted Weekly Hours  Implemented Weekly Hours   diff  diff_perc  \\\n",
      "0                      NaN                       NaN    NaN        NaN   \n",
      "1                      NaN                       NaN    NaN        NaN   \n",
      "2                    0.000                      0.00  0.000        NaN   \n",
      "3                    2.325                      2.35 -0.025    -0.0108   \n",
      "4                    6.977                      6.99 -0.013    -0.0019   \n",
      "\n",
      "   age_closure_prob  \n",
      "0               NaN  \n",
      "1               NaN  \n",
      "2               NaN  \n",
      "3               NaN  \n",
      "4               NaN  \n"
     ]
    }
   ],
   "source": [
    "if not win_rate_csv:\n",
    "    # merge open_opps_with_probs with win_probs_hours_conversion\n",
    "    win_probs_hours_conversion_closing_age = win_probs_hours_conversion.merge(open_opps_with_probs, on='Category', how='left')\n",
    "    \n",
    "    print(win_probs_hours_conversion_closing_age.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not win_rate_csv:\n",
    "    # bringing back to the rest of the data\n",
    "    supply_demand_all_products_pipe_raw_win_merged = supply_demand_all_products_pipe_raw.merge(win_probs_hours_conversion_closing_age, \n",
    "                                                                                               left_on='Product Category', right_on='Category', how='left'\n",
    "                                                                                               ).drop(columns=['Category'])\n",
    "    supply_demand_all_products_pipe_raw_win_merged.loc[:, \"Win Rate\"] = np.nanprod(\n",
    "        supply_demand_all_products_pipe_raw_win_merged[['age_closure_prob', 'Predicted Win Probability']],\n",
    "        axis=1\n",
    "    )\n",
    "    # if both 'age_closure_prob' and 'Predicted Win Probability' are null, then the win rate is null\n",
    "    supply_demand_all_products_pipe_raw_win_merged['Win Rate'] = np.where(\n",
    "        (supply_demand_all_products_pipe_raw_win_merged['age_closure_prob'].isnull() &\n",
    "        supply_demand_all_products_pipe_raw_win_merged['Predicted Win Probability'].isnull()),\n",
    "        np.nan,\n",
    "        supply_demand_all_products_pipe_raw_win_merged['Win Rate']\n",
    "    )\n",
    "    # fill na with 0.1\n",
    "    supply_demand_all_products_pipe_raw_win_merged['Win Rate'] = supply_demand_all_products_pipe_raw_win_merged['Win Rate'].fillna(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjusting Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Product Category  Demand_FTE  Capacity_FTE  Pipe Raw  \\\n",
      "0            ACD CP        0.30           NaN       NaN   \n",
      "1            ACD HF        0.03          0.28       NaN   \n",
      "2     AFib Ablation         NaN           NaN  0.241725   \n",
      "3              AJRR         NaN          0.02  0.081375   \n",
      "4  Advisory - Spark         NaN           NaN  0.000000   \n",
      "\n",
      "   Predicted Win Probability  Contracted Weekly Hours  \\\n",
      "0                        NaN                      NaN   \n",
      "1                        NaN                      NaN   \n",
      "2                   0.218357                      NaN   \n",
      "3                   0.003175                      NaN   \n",
      "4                   0.130506                      0.0   \n",
      "\n",
      "   Implemented Weekly Hours  diff  diff_perc  age_closure_prob  Win Rate  \\\n",
      "0                       NaN   NaN        NaN               NaN  0.100000   \n",
      "1                       NaN   NaN        NaN               NaN  0.100000   \n",
      "2                       NaN   NaN        NaN               NaN  0.218357   \n",
      "3                       NaN   NaN        NaN               NaN  0.003175   \n",
      "4                       0.0   0.0        NaN               NaN  0.130506   \n",
      "\n",
      "   Pipeline Adjusted  \n",
      "0                NaN  \n",
      "1                NaN  \n",
      "2           0.052782  \n",
      "3           0.000258  \n",
      "4           0.000000  \n"
     ]
    }
   ],
   "source": [
    "# Pipeline Adjusted\n",
    "if win_rate_csv:\n",
    "    supply_demand_all_products_pipe_raw_win_merged.loc[:, \"Pipeline Adjusted\"] = supply_demand_all_products_pipe_raw_win_merged['Pipe Raw'] * \\\n",
    "    supply_demand_all_products_pipe_raw_win_merged['Win Rate']\n",
    "    print(supply_demand_all_products_pipe_raw_win_merged.head())\n",
    "else:\n",
    "    # this calculates the new Pipeline adjusted column, \n",
    "    # it multiplies the Pipe raw by the win rate then since Pipe Raw is negative it subtracts the amount that we can \n",
    "    # expect when you multiply the Pipe Raw by the average percent difference between \n",
    "    # the historic Contracted Weekly hours and the actual implemented weekly hours\n",
    "    supply_demand_all_products_pipe_raw_win_merged.loc[:, \"Pipeline Adjusted\"] = (supply_demand_all_products_pipe_raw_win_merged['Pipe Raw'] * \\\n",
    "    supply_demand_all_products_pipe_raw_win_merged['Win Rate']) + \\\n",
    "        supply_demand_all_products_pipe_raw_win_merged[['diff_perc', 'Pipe Raw']].apply(prod_with_nan_as_zero, axis=1)\n",
    "    print(supply_demand_all_products_pipe_raw_win_merged.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rolling Attrition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_last_day = datetime.now().date()\n",
    "monthly_last_day = datetime.now().date() - timedelta(days=datetime.now().day)\n",
    "# get all the contacts who their last day was in the last 3 months\n",
    "ab_rolling = queries.convert_salesforce_data_to_df(queries.sf.query(queries.create_query(\n",
    "    columns=\"Name,External_Employee_ID__c,Id,Last_Date__c,Full_Time_Status__c,Resource_Role__c,Group_Name__c,Week_Hours__c,Business_Leader__r.Name\",\n",
    "    table=\"Contact\", \n",
    "    conditions={\"AccountId\": \"001j000000i0d9tAAA\",\n",
    "                \"Last_Date__c >\": (datetime.now() - relativedelta(months=3)).date(), \n",
    "                \"Last_Date__c <\": rolling_last_day,\n",
    "                \"QC_Active__c\": False,\n",
    "                \"PT_Action_Plan__c !\": \"Plan to Separate\"})))#['records']).drop(['attributes'],axis=1)\n",
    "ab_monthly = queries.convert_salesforce_data_to_df(queries.sf.query(queries.create_query(\n",
    "    columns=\"Name,External_Employee_ID__c,Id,Last_Date__c,Full_Time_Status__c,Resource_Role__c,Group_Name__c,Week_Hours__c,Business_Leader__r.Name\",\n",
    "    table=\"Contact\", \n",
    "    conditions={\"AccountId\": \"001j000000i0d9tAAA\",\n",
    "                \"Last_Date__c >\": (last_day_of_month((datetime.now() - relativedelta(months=4)).strftime(\"%Y-%m-%d\")) + relativedelta(days=1)).date(), \n",
    "                \"Last_Date__c <\": monthly_last_day,\n",
    "                \"QC_Active__c\": False,\n",
    "                \"PT_Action_Plan__c !\": \"Plan to Separate\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was an error with Farrah Scodius: \"['attributes'] not found in axis\"\n",
      "There was an error with Lori Kinnunen: \"['attributes'] not found in axis\"\n",
      "There was an error with Farrah Scodius: \"['attributes'] not found in axis\"\n",
      "There was an error with Lori Kinnunen: \"['attributes'] not found in axis\"\n"
     ]
    }
   ],
   "source": [
    "def attrition_categories_fx(abstractors, last_day_of_month):\n",
    "    abstractors = abstractors[(abstractors['Resource_Role__c'] == 'Senior Clinical Data Specialist') | \n",
    "                            (abstractors['Resource_Role__c'] == 'Clinical Data Support Specialist') |\n",
    "                            (abstractors['Resource_Role__c'] == 'Clinical Data Specialist')]\n",
    "    # convert Week_Hours__c to int64\n",
    "    abstractors.loc[:, 'Week_Hours__c'] = abstractors['Week_Hours__c'].astype('Int64')\n",
    "    # if any abstractors have a Full_Time_Status__c of PT and a Business_Leader__r.Name of Paul Gasque and are not in the \"Infection Prevention\" group then divide \n",
    "    # their 'Week_Hours__c' by 2\n",
    "    abstractors.loc[(abstractors['Full_Time_Status__c'] == 'PT') &\n",
    "                    (abstractors['Business_Leader__r.Name'] == 'Paul Gasque') &\n",
    "                    (abstractors['Group_Name__c'] != 'Infection Prevention'), 'Week_Hours__c'] = abstractors['Week_Hours__c'] / 2\n",
    "    skills_report = rf.get_report(\"00Oan0000033WclEAE\", id_column='row_id')\n",
    "    skills_report_abs_of_interest = skills_report[skills_report['Contact ID (Case Safe)'].isin(abstractors['Id'].unique())].copy()\n",
    "    valid_skills = skills_report_abs_of_interest[skills_report_abs_of_interest['Product Skill Rating'].str.contains(r\"0|1|2|6\")].copy()\n",
    "    valid_skills_dups_drp = valid_skills[['Contact ID (Case Safe)', 'Related Product Skill Category']].drop_duplicates()\n",
    "    # groupby cbiz_name and concatenate the 'Related Product Skill Category'\n",
    "    valid_skills_current_skills = valid_skills_dups_drp.groupby('Contact ID (Case Safe)')['Related Product Skill Category']\\\n",
    "        .apply(lambda x: \",\".join(x)).reset_index()\n",
    "\n",
    "    attrition_in_these_cats_all = None\n",
    "    attrition_in_these_cats_all_30 = None\n",
    "    check_dict = {}\n",
    "    # loop through all the rows in abstractors\n",
    "    for i in range(len(abstractors)):\n",
    "        group_name = abstractors.iloc[i, list(abstractors.columns).index(\"Group_Name__c\")]\n",
    "        name = abstractors.iloc[i, list(abstractors.columns).index(\"Name\")]\n",
    "        # get the last_day and abstractor\n",
    "        last_day = abstractors.iloc[i, list(abstractors.columns).index(\"Last_Date__c\")]\n",
    "        id = abstractors.iloc[i, list(abstractors.columns).index(\"Id\")]\n",
    "        role = abstractors.iloc[i, list(abstractors.columns).index(\"Resource_Role__c\")]\n",
    "        hours = abstractors.iloc[i, list(abstractors.columns).index(\"Week_Hours__c\")]\n",
    "        # convert last_day to datetime\n",
    "        last_day = datetime.strptime(last_day, '%Y-%m-%d')\n",
    "        try:\n",
    "            assignments = queries.convert_salesforce_data_to_df(queries.sf.query(queries.create_query(\n",
    "                columns=\"Id,Resource__r.Name,QC_Team__r.Name,Planned_Hours__c,Team_Position__c,End_Date__c,Team_Category__c\",\n",
    "                table=\"Assignment__c\", \n",
    "                conditions={#\"End_Date__c >\": (last_day - timedelta(days=8)).date(),\n",
    "                            \"Planned_Hours__c >\": 0.5,\n",
    "                            \"Resource__r.Id\": id})))\n",
    "            # filter out any assignments that are not in their current skillset and have a value of 0,1,2,or 6\n",
    "            assignments = assignments[(assignments['Team_Category__c'].isin(\n",
    "                valid_skills_current_skills.loc[valid_skills_current_skills['Contact ID (Case Safe)'] == id, \n",
    "                                                'Related Product Skill Category'].str.split(\",\").values[0]))]\n",
    "        except Exception as e:\n",
    "            print(f'There was an error with {abstractors.iloc[i, list(abstractors.columns).index(\"Name\")]}: {e}')\n",
    "            attrition_in_these_cats = pd.DataFrame(data={'Planned_Hours__c':0, 'total_hours_planned':0, 'proportion':0, 'Hours_Lost':hours},\n",
    "                                                    columns=['Planned_Hours__c', 'total_hours_planned', 'proportion', 'Hours_Lost'], \n",
    "                                                index=valid_skills_current_skills.loc[\n",
    "                                                    valid_skills_current_skills['Contact ID (Case Safe)'] == id, \n",
    "                                                    'Related Product Skill Category'].str.split(\",\").values[0])\n",
    "            if (group_name == 'Cancer') & (role == 'Clinical Data Support Specialist'):\n",
    "                # replace any value in the index that is 'Cancer' with 'Cancer - CDSS'\n",
    "                attrition_in_these_cats.index = attrition_in_these_cats.index.map(lambda x: x if x != 'Cancer' else\n",
    "                                                                                'Cancer - CDSS')\n",
    "            elif (group_name == 'Cancer') & (role != 'Clinical Data Support Specialist'):\n",
    "                attrition_in_these_cats.index = attrition_in_these_cats.index.map(lambda x: x if x != 'Cancer' else\n",
    "                                                                                'Cancer - Abstractor')\n",
    "            # print(attrition_in_these_cats)\n",
    "            try:\n",
    "                attrition_in_these_cats_all = pd.concat([attrition_in_these_cats_all, attrition_in_these_cats])\n",
    "            except:\n",
    "                attrition_in_these_cats_all = attrition_in_these_cats\n",
    "            if last_day.date() >= (last_day_of_month - relativedelta(months=1)):\n",
    "                try:\n",
    "                    attrition_in_these_cats_all_30 = pd.concat([attrition_in_these_cats_all_30, attrition_in_these_cats])\n",
    "                except:\n",
    "                    attrition_in_these_cats_all_30 = attrition_in_these_cats\n",
    "            continue\n",
    "        assignments.loc[:, \"Team_Category__c\"] = np.where(\n",
    "            (role == 'Clinical Data Support Specialist') & (assignments[\"Team_Category__c\"] == \"Cancer\"), \"Cancer - CDSS\", \n",
    "            np.where((role != 'Clinical Data Support Specialist') & (assignments[\"Team_Category__c\"] == \"Cancer\"), \"Cancer - Abstractor\", \n",
    "                    assignments[\"Team_Category__c\"]))\n",
    "        assignments.loc[:, 'total_hours_planned'] = assignments['Planned_Hours__c'].sum()\n",
    "\n",
    "        # groupby team_category__c and find the proportion of planned_hours__c per category out of the total_hours_planned\n",
    "        attrition_in_these_cats = assignments.groupby(\"Team_Category__c\").agg({\n",
    "            'Planned_Hours__c': 'sum','total_hours_planned': 'first'}).assign(\n",
    "                proportion = lambda x: x['Planned_Hours__c'] / x['total_hours_planned'] if x['total_hours_planned'].all() != 0 else 0)\n",
    "\n",
    "        # multiply proportion by hours to get the number of hours per category\n",
    "        attrition_in_these_cats.loc[:, \"Hours_Lost\"] = attrition_in_these_cats['proportion'] * hours\n",
    "        if group_name.strip() == 'Cancer':\n",
    "            check_dict[name] = attrition_in_these_cats\n",
    "            # print(attrition_in_these_cats)\n",
    "        try:\n",
    "            attrition_in_these_cats_all = pd.concat([attrition_in_these_cats_all, attrition_in_these_cats])\n",
    "        except:\n",
    "            attrition_in_these_cats_all = attrition_in_these_cats\n",
    "        if last_day.date() >= (last_day_of_month - relativedelta(months=1)):\n",
    "            try:\n",
    "                attrition_in_these_cats_all_30 = pd.concat([attrition_in_these_cats_all_30, attrition_in_these_cats])\n",
    "            except:\n",
    "                attrition_in_these_cats_all_30 = attrition_in_these_cats\n",
    "    # regroup by the index and find the sum of Planned_Hours__c\n",
    "    attrition_in_these_cats_all_gr = attrition_in_these_cats_all.groupby(attrition_in_these_cats_all.index).sum()[['Hours_Lost']]\n",
    "    attrition_in_these_cats_all_gr = attrition_in_these_cats_all_gr.rename(columns={\"Hours_Lost\": \"Attrit, [n]m\"})\n",
    "    # divide by 40 for conversion to FTE and 3 for monthly calc\n",
    "    attrition_in_these_cats_all_gr.loc[:, \"Attrit, [n]m\"] = attrition_in_these_cats_all_gr['Attrit, [n]m'] / 40 / 3\n",
    "    attrition_in_these_cats_all_30_gr = attrition_in_these_cats_all_30.groupby(attrition_in_these_cats_all_30.index).sum()[['Hours_Lost']]\n",
    "    attrition_in_these_cats_all_30_gr = attrition_in_these_cats_all_30_gr.rename(columns={\"Hours_Lost\": \"30-days Attrit, [n]m\"})\n",
    "    # divide by 40 for conversion to FTE\n",
    "    attrition_in_these_cats_all_30_gr.loc[:, \"30-days Attrit, [n]m\"] = attrition_in_these_cats_all_30_gr['30-days Attrit, [n]m'] / 40\n",
    "\n",
    "    return attrition_in_these_cats_all_gr, attrition_in_these_cats_all_30_gr\n",
    "\n",
    "attrition_in_these_cats_all_gr_rolling, attrition_in_these_cats_all_30_gr_rolling = attrition_categories_fx(ab_rolling, rolling_last_day)\n",
    "attrition_in_these_cats_all_gr_monthly, attrition_in_these_cats_all_30_gr_monthly = attrition_categories_fx(ab_monthly, monthly_last_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling attrition joining\n",
    "supply_demand_all_products_pipe_merged_1_rolling = pd.merge(supply_demand_all_products_pipe_raw_win_merged, attrition_in_these_cats_all_gr_rolling, \n",
    "                                               left_on='Product Category', right_index=True, how='left')#.drop('Category', axis=1)\n",
    "supply_demand_all_products_pipe_merged_rolling = pd.merge(supply_demand_all_products_pipe_merged_1_rolling, attrition_in_these_cats_all_30_gr_rolling, \n",
    "                                               left_on='Product Category', right_index=True, how='left')#.drop('Category', axis=1)\n",
    "# monthly attrition join\n",
    "supply_demand_all_products_pipe_merged_1_monthly = pd.merge(supply_demand_all_products_pipe_raw_win_merged, attrition_in_these_cats_all_gr_monthly, \n",
    "                                               left_on='Product Category', right_index=True, how='left')#.drop('Category', axis=1)\n",
    "supply_demand_all_products_pipe_merged_monthly = pd.merge(supply_demand_all_products_pipe_merged_1_monthly, attrition_in_these_cats_all_30_gr_monthly, \n",
    "                                               left_on='Product Category', right_index=True, how='left')#.drop('Category', axis=1)\n",
    "\n",
    "# replace all of the NaN with 0\n",
    "supply_demand_all_products_pipe_adj_rolling = final_column_adj_fx(supply_demand_all_products_pipe_merged_rolling)\n",
    "supply_demand_all_products_pipe_adj_monthly = final_column_adj_fx(supply_demand_all_products_pipe_merged_monthly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "if win_rate_csv:\n",
    "    filename = f\"{path_to_staffing_models_folder}staffing_model_{today}_csv_wr.xlsx\"\n",
    "else:\n",
    "    filename = f\"{path_to_staffing_models_folder}staffing_model_{today}_calc_wr.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter(filename, engine='xlsxwriter')\n",
    "supply_demand_all_products_pipe_adj_rolling.to_excel(writer, sheet_name='Rolling Attrition', index=False)\n",
    "supply_demand_all_products_pipe_adj_monthly.to_excel(writer, sheet_name='Monthly Attrition', index=False)\n",
    "writer.close()\n",
    "create_capbase.adjust_workbook_column_widths(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
