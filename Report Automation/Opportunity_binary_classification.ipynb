{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Hours Analysis: Predicting Business Implementation from Pipeline Data\n",
    "\n",
    "## Overview\n",
    "This notebook explores methods to predict the conversion rate of pipeline opportunities into actual implemented business hours. The analysis aims to help business planning by providing better forecasting of resource requirements based on current pipeline data. This notebook was mainly EDA and exploration into the space to see if classification or regression models would help in our forecasting efforts. In the end, the best choice was to use the monthly snapshot approach, which approximated the hours needed the best out of any more sophisticated model with much more simplicity. \n",
    "\n",
    "## Project Goals\n",
    "- Develop models to predict which opportunities will close successfully\n",
    "- Analyze the timeline of opportunity closures\n",
    "- Calculate the relationship between contracted and implemented hours\n",
    "- Create monthly snapshots of pipeline conversion rates\n",
    "- Provide insights for resource planning and capacity management\n",
    "\n",
    "## Technical Approach\n",
    "The analysis employs several sophisticated methods:\n",
    "1. **Binary Classification Models**\n",
    "   - Random Forest and XGBoost classifiers to predict opportunity success\n",
    "   - Feature engineering including opportunity age, type, category, and contract values\n",
    "   - SMOTE for handling class imbalance\n",
    "\n",
    "2. **Survival Analysis**\n",
    "   - Kaplan-Meier estimator for analyzing opportunity lifecycle\n",
    "   - Cox Proportional Hazards model for understanding factors affecting closure times\n",
    "   - Time-based binning (0-30 days, 31-60 days, 60+ days) for practical planning\n",
    "\n",
    "3. **Pipeline Conversion Analysis**\n",
    "   - Monthly snapshot creation of contracted vs. implemented hours\n",
    "   - Category-wise analysis of conversion rates\n",
    "   - Historical trending of implementation rates\n",
    "\n",
    "## Data Sources\n",
    "The analysis uses Salesforce opportunity data, including:\n",
    "- Opportunity details (ID, Type, Category)\n",
    "- Timeline information (Start Date, Close Date)\n",
    "- Contract values (Weekly Hours, ACV)\n",
    "- Implementation metrics\n",
    "- Status and stage information\n",
    "\n",
    "## Business Impact\n",
    "This analysis helps answer critical business questions:\n",
    "- What percentage of pipeline opportunities will convert to actual business?\n",
    "- When are opportunities likely to close?\n",
    "- How many resources should be prepared for upcoming implementations?\n",
    "- What is the typical gap between contracted and implemented hours?\n",
    "\n",
    "These insights enable more accurate resource planning and improve operational efficiency by better aligning capacity with expected demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, classification_report, roc_auc_score, accuracy_score, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from datetime import datetime\n",
    "# Import the module under a specific name\n",
    "import importlib\n",
    "import sf_queries_class\n",
    "importlib.reload(sf_queries_class)\n",
    "from sf_queries_class import SfQueries\n",
    "import my_sf_secrets\n",
    "import capacity_portion\n",
    "importlib.reload(capacity_portion)\n",
    "from reportforce import Reportforce\n",
    "from lifelines.statistics import logrank_test\n",
    "from xgboost import XGBClassifier\n",
    "import statsmodels.api as sm\n",
    "from lifelines import KaplanMeierFitter, CoxPHFitter\n",
    "my_sf_username, my_sf_password, my_sf_security_token = my_sf_secrets.get_my_sf_secrets()\n",
    "queries = SfQueries(\n",
    "    username=my_sf_username,\n",
    "    password=my_sf_password,\n",
    "    security_token=my_sf_security_token\n",
    ")\n",
    "\n",
    "rf = Reportforce(session_id=queries.sf.session_id, instance_url=queries.sf.sf_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'ofph' not in globals():\n",
    "    print(\"You need OFPH\")\n",
    "    ofph = rf.get_report(\"ReportID\", id_column='Opportunity ID')\n",
    "len(ofph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ofph_fil = ofph[(ofph['Type'].isin(['New Client', 'Existing Client - New Service Line', 'Existing Client - Service Line Expansion'])) &\n",
    "                 (ofph['Exclude from Resource Requests'] == 'false') &\n",
    "                 (ofph['Close Date'] <= datetime.now())].copy()\n",
    "len(ofph_fil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column that is called 'Won' if `Probability (%)` is > 0 otherwise 'Lost'\n",
    "ofph_fil.loc[:, 'Won'] = (ofph_fil['Probability (%)'] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_identifier = ['Opportunity ID'] # String\n",
    "\n",
    "# removed Opportunity Owner, Created Date, Close Date\n",
    "important_features = ['Opportunity ID', ## String\n",
    "                      'Age', # Int\n",
    "                      'Type', # String\n",
    "                      'Category', # String\n",
    "                      'Contracted Weekly Hours', # Float\n",
    "                      'Backlog', # Bool\n",
    "                      'Won', # Int\n",
    "                      'Contracted Opportunity ACV', # Float\n",
    "                      'Hospital Count', # Int\n",
    "                    #   'Implemented Weekly Hours', # Float choosing Contracted for classification\n",
    "                      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ofph_index_reset = ofph_fil.reset_index()\n",
    "# convert the Backlog string column to a boolean by changing the true values to 1 and the false values to 0\n",
    "ofph_index_reset['Backlog'] = ofph_index_reset['Backlog'].str.replace('true', '1').str.replace('false', '0').astype(int)\n",
    "ofph_index_reset_sel = ofph_index_reset[important_features]\n",
    "ofph_index_reset_sel = ofph_index_reset_sel.dropna()\n",
    "ofph_index_reset_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "ofph_index_reset_sel.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ofph_index_reset_sel.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply one-hot encoding to the 'Category' column\n",
    "encoded_df = pd.get_dummies(ofph_index_reset_sel, columns=['Type','Category'], drop_first=True)\n",
    "\n",
    "\n",
    "# Display the encoded DataFrame\n",
    "encoded_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns to normalize\n",
    "columns_to_normalize = [\n",
    "    'Contracted Opportunity ACV',\n",
    "    'Contracted Weekly Hours',\n",
    "    'Hospital Count',\n",
    "    'Age'\n",
    "]\n",
    "# groupby Opportunity ID and get the first value from 'Age', 'Backlog', and 'Hospital Count'\n",
    "abhc = encoded_df.groupby('Opportunity ID').first()[['Age', 'Backlog', 'Hospital Count', 'Contracted Opportunity ACV']]\n",
    "# group by Opportunity ID and calculate the sum for all the other columns\n",
    "rest = encoded_df.groupby('Opportunity ID').sum().drop(['Age', 'Backlog', 'Hospital Count', 'Contracted Opportunity ACV'], axis=1)\n",
    "# merge the 2 back together\n",
    "encoded_df_merged = pd.merge(abhc, rest, left_index=True, right_index=True)\n",
    "# in all columns except for 'Age', 'Backlog', 'Hospital Count', 'Contracted Weekly Hours', and 'Contracted Opportunity ACV', replace any number greater than 0 with 1\n",
    "cols_not_to_change = ['Age', 'Backlog', 'Hospital Count', 'Contracted Weekly Hours', 'Contracted Opportunity ACV']\n",
    "for col in encoded_df_merged.columns:\n",
    "    if col not in cols_not_to_change:\n",
    "        encoded_df_merged[col] = encoded_df_merged[col].apply(lambda x: 1 if x > 0 else 0)\n",
    "encoded_df_base = encoded_df_merged.copy()\n",
    "encoded_df_merged_reg = encoded_df_merged.copy()\n",
    "# Fit and transform the selected columns\n",
    "encoded_df_merged[columns_to_normalize] = scaler.fit_transform(encoded_df_merged[columns_to_normalize])\n",
    "cols_for_reg = [\n",
    "    'Contracted Opportunity ACV',\n",
    "    'Contracted Weekly Hours',\n",
    "    'Hospital Count']\n",
    "encoded_df_merged_reg[cols_for_reg] = scaler.fit_transform(encoded_df_merged_reg[cols_for_reg])\n",
    "\n",
    "encoded_df_merged_index_reset = encoded_df_merged.reset_index()\n",
    "encoded_df_merged_reg_index_reset = encoded_df_merged_reg.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Win Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target variable\n",
    "target = 'Won'\n",
    "\n",
    "# Split the data\n",
    "X = encoded_df_merged_index_reset.drop(columns=[target, 'Opportunity ID'])\n",
    "y = encoded_df_merged_index_reset[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = XGBClassifier(eval_metric='logloss')\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('AUC-ROC:', roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get probabilities\n",
    "probabilities = model.predict_proba(X_test)\n",
    "\n",
    "# Get the probability of the positive class (usually class 1)\n",
    "positive_class_probs = probabilities[:, 1]\n",
    "\n",
    "# You can then use these probabilities for various purposes\n",
    "for i, prob in zip(X_test.index, positive_class_probs):\n",
    "    print(f\"Sample {i}: Probability of positive class = {prob:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The XGBoost model tells us the probability that we will win the opportunity. Given that we win, now we need to determine what is the age that the opportunity will close at and how many hours will be implemented versus what is contracted for. \n",
    "\n",
    "I think figuring out how much is implemented versus what is contracted for is an easier task to determine. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implemented Vs Contracted Hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_imp = ofph_fil.reset_index().loc[(ofph_fil.reset_index()['Probability (%)'] > 0), ['Category', 'Contracted Weekly Hours', 'Implemented Weekly Hours']]\\\n",
    "    .groupby('Category').sum()[['Contracted Weekly Hours', 'Implemented Weekly Hours']]\n",
    "con_imp.loc[:, 'diff'] = con_imp['Implemented Weekly Hours'] - con_imp['Contracted Weekly Hours']\n",
    "con_imp.loc[:, 'diff_perc'] = round(con_imp['diff'] / con_imp['Contracted Weekly Hours'], 4)\n",
    "con_imp.loc[:, 'diff'] = round(con_imp.loc[:, 'diff'], 4)\n",
    "# con_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we have a percentage that given we win an opportunity, how much of that opportunity we will actually recognize in implemented hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closing Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_df_merged_reg_index_reset['Age_logged'] = np.log1p(encoded_df_merged_reg_index_reset['Age'])\n",
    "features_i = encoded_df_merged_reg_index_reset.columns\n",
    "features = [f for f in features_i if f not in ['Opportunity ID', 'Age_logged', 'Won'] and not f.startswith('Category_')]\n",
    "\n",
    "def prepare_data(df):\n",
    "    X = df[features]\n",
    "    y = df['Age_logged']\n",
    "    return train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_won, X_test_won, y_train_won, y_test_won = prepare_data(encoded_df_merged_reg_index_reset[encoded_df_merged_reg_index_reset['Won'] == 1])\n",
    "X_train_lost, X_test_lost, y_train_lost, y_test_lost = prepare_data(encoded_df_merged_reg_index_reset[encoded_df_merged_reg_index_reset['Won'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a random forest regressor model\n",
    "rf_won = RandomForestRegressor(n_estimators = 136, max_depth = 9, min_samples_split = 4, max_features = 'sqrt', random_state=42) \n",
    "# {'n_estimators': 136, 'max_depth': 9, 'min_samples_split': 4, 'max_features': 'sqrt'}\n",
    "rf_lost = RandomForestRegressor(n_estimators = 77, max_depth = 5, min_samples_split = 3, max_features = 'sqrt', random_state=42) \n",
    "# {'n_estimators': 77, 'max_depth': 5, 'min_samples_split': 3, 'max_features': 'sqrt'}\n",
    "\n",
    "rf_won.fit(X_train_won, y_train_won)\n",
    "rf_lost.fit(X_train_lost, y_train_lost)\n",
    "\n",
    "y_pred_won = rf_won.predict(X_test_won)\n",
    "y_pred_lost = rf_lost.predict(X_test_lost)\n",
    "\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"{model_name} - RMSE: {rmse:.2f}, R2 Score: {r2:.2f}\")\n",
    "\n",
    "evaluate_model(y_test_won, y_pred_won, \"Won Opportunities Model\")\n",
    "evaluate_model(y_test_lost, y_pred_lost, \"Lost Opportunities Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the predictions versus the true values\n",
    "plt.scatter(y_test_won, y_pred_won)\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(model, feature_names):\n",
    "    importances = model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.title(\"Feature Importances\")\n",
    "    plt.bar(range(len(importances)), importances[indices])\n",
    "    plt.xticks(range(len(importances)), [feature_names[i] for i in indices], rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_feature_importance(rf_won, features)\n",
    "plot_feature_importance(rf_lost, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def learning_curve_pandas(rf, X_train, y_train, X_test, y_test, train_sizes):\n",
    "    \"\"\"\n",
    "    Manually plots learning curves for a given sklearn model.\n",
    "\n",
    "    Args:\n",
    "        rf: The sklearn Random Forest model instance.\n",
    "        X_train: The training features.\n",
    "        y_train: The training target.\n",
    "        X_test: The testing features.\n",
    "        y_test: The testing target.\n",
    "        train_sizes: Relative or absolute numbers of training examples to use for generating the learning curve.\n",
    "    \"\"\"\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "\n",
    "    for size in train_sizes:\n",
    "        # Sample the training data according to the current size\n",
    "        sample_size = int(len(X_train) * size)\n",
    "        X_sample, y_sample = X_train[:sample_size], y_train[:sample_size]\n",
    "        \n",
    "        # Train the model\n",
    "        rf.fit(X_sample, y_sample)\n",
    "        \n",
    "        # Make predictions and evaluate the model on both training and testing data\n",
    "        train_pred = rf.predict(X_sample)\n",
    "        test_pred = rf.predict(X_test)\n",
    "        \n",
    "        train_rmse = np.sqrt(mean_squared_error(y_sample, train_pred))\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "        \n",
    "        train_scores.append(train_rmse)\n",
    "        test_scores.append(test_rmse)\n",
    "\n",
    "    return train_scores, test_scores\n",
    "\n",
    "def train_and_evaluate_rf_optuna(X_train, X_test, y_train, y_test):\n",
    "    def objective(trial):\n",
    "        # Define the hyperparameter search space\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 50, 200)\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 5, 15)\n",
    "        min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n",
    "        max_features = trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\"])\n",
    "\n",
    "        # Create a Random Forest model with the suggested hyperparameters\n",
    "        rf = RandomForestRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            max_features=max_features,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        rf.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions on the validation data\n",
    "        predictions = rf.predict(X_test)\n",
    "\n",
    "        # Evaluate the model using RMSE\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "\n",
    "        return rmse\n",
    "\n",
    "    # Create an Optuna study\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=100)  # Adjust the number of trials as needed\n",
    "\n",
    "    # Get the best hyperparameters and RMSE\n",
    "    best_params = study.best_trial.params\n",
    "    best_rmse = study.best_value\n",
    "    print(f\"Best Hyperparameters: {best_params}\\nBest RMSE: {best_rmse}\")\n",
    "\n",
    "    # Train the final model with the best hyperparameters\n",
    "    best_rf = RandomForestRegressor(\n",
    "        n_estimators=best_params[\"n_estimators\"],\n",
    "        max_depth=best_params[\"max_depth\"],\n",
    "        min_samples_split=best_params[\"min_samples_split\"],\n",
    "        max_features=best_params[\"max_features\"],\n",
    "        random_state=42\n",
    "    )\n",
    "    best_rf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the testing data\n",
    "    predictions = best_rf.predict(X_test)\n",
    "\n",
    "    # Extract feature importances\n",
    "    feature_importances = best_rf.feature_importances_\n",
    "\n",
    "    # Return the best model, feature importances, and predictions\n",
    "    return best_rf, feature_importances, predictions\n",
    "\n",
    "def plot_figs(X_train, X_test, y_train, y_test, input_cols):\n",
    "    print(f\"\\n{'='*50}\\nProcessing output column: Age\\n{'='*50}\")\n",
    "    \n",
    "    rf_model, feature_importances, predictions = train_and_evaluate_rf_optuna(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    residuals = y_test - predictions\n",
    "    \n",
    "    # Summary Statistics of Residuals\n",
    "    print(\"\\nSummary Statistics of Residuals:\")\n",
    "    print(pd.Series(residuals).describe())\n",
    "\n",
    "    # Create a single figure with 6 subplots\n",
    "    fig, axs = plt.subplots(3, 2, figsize=(20, 24))\n",
    "    fig.suptitle(\"Analysis for Age\", fontsize=16)\n",
    "\n",
    "    # 1. Learning Curve\n",
    "    train_sizes = np.linspace(0.1, 1.0, 5)\n",
    "    train_scores, test_scores = learning_curve_pandas(rf_model, X_train, y_train, X_test, y_test, train_sizes)\n",
    "\n",
    "    axs[0, 0].plot(train_sizes, train_scores, label='Train RMSE')\n",
    "    axs[0, 0].plot(train_sizes, test_scores, label='Test RMSE')\n",
    "    axs[0, 0].set_xlabel('Training Examples')\n",
    "    axs[0, 0].set_ylabel('RMSE')\n",
    "    axs[0, 0].set_title('Learning Curves')\n",
    "    axs[0, 0].legend()\n",
    "    axs[0, 0].grid(True)\n",
    "\n",
    "    # 2. Feature Importances\n",
    "    axs[0, 1].bar(range(len(input_cols)), feature_importances)\n",
    "    axs[0, 1].set_xticks(range(len(input_cols)))\n",
    "    axs[0, 1].set_xticklabels(input_cols, rotation=90)\n",
    "    axs[0, 1].set_xlabel(\"Features\")\n",
    "    axs[0, 1].set_ylabel(\"Importance\")\n",
    "    axs[0, 1].set_title(\"Feature Importances\")\n",
    "\n",
    "    # 3. Test vs Predictions\n",
    "    axs[1, 0].scatter(y_test, predictions)\n",
    "    axs[1, 0].set_xlabel(\"Actual Age\")\n",
    "    axs[1, 0].set_ylabel(\"Predicted Age\")\n",
    "    axs[1, 0].set_title(\"Test vs Predictions\")\n",
    "\n",
    "    # 4. Residual Plot\n",
    "    axs[1, 1].scatter(predictions, residuals)\n",
    "    axs[1, 1].axhline(y=0, color='r', linestyle='--')\n",
    "    axs[1, 1].set_xlabel(\"Predicted Values\")\n",
    "    axs[1, 1].set_ylabel(\"Residuals\")\n",
    "    axs[1, 1].set_title(\"Residual Plot\")\n",
    "\n",
    "    # 5. Histogram of Residuals\n",
    "    axs[2, 0].hist(residuals, bins=20)\n",
    "    axs[2, 0].set_xlabel(\"Residuals\")\n",
    "    axs[2, 0].set_ylabel(\"Frequency\")\n",
    "    axs[2, 0].set_title(\"Histogram of Residuals\")\n",
    "\n",
    "    # 6. Q-Q Plot\n",
    "    sm.qqplot(pd.Series(residuals), line='s', ax=axs[2, 1])\n",
    "    axs[2, 1].set_title(\"Q-Q Plot\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming you have your data in a pandas DataFrame called 'df'\n",
    "# with 'Age' as the target variable and other columns as features\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Prepare your data\n",
    "# X = df.drop('Age', axis=1)\n",
    "# y = df['Age']\n",
    "# input_cols = X.columns.tolist()\n",
    "\n",
    "# Split the data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Run the analysis\n",
    "plot_figs(X_train_won, X_test_won, y_train_won, y_test_won, features)\n",
    "plot_figs(X_train_lost, X_test_lost, y_train_lost, y_test_lost, features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bins and corresponding labels\n",
    "bins = [0, 30, 60, float('inf')]\n",
    "labels = ['0-30 days', '31-60 days', '> 61 days']\n",
    "\n",
    "# Create a new column 'Age_Bin' based on the bins\n",
    "encoded_df_merged_reg_index_reset['Age_Bin'] = pd.cut(encoded_df_merged_reg_index_reset['Age'], bins=bins, labels=labels, right=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_df_merged_reg_index_reset['Age_Bin'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the feature list to exclude 'Age' and include the binned target variable 'Age_Bin'\n",
    "features_i = encoded_df_merged_reg_index_reset.columns\n",
    "features = [f for f in features_i if f not in ['Opportunity ID', 'Age', 'Won', 'Age_Bin']] #  and not f.startswith('Category_')\n",
    "\n",
    "def prepare_data(df):\n",
    "    X = df[features]\n",
    "    y = df['Age_Bin']\n",
    "    return train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare training and testing sets based on the new target\n",
    "X_train_won, X_test_won, y_train_won, y_test_won = prepare_data(encoded_df_merged_reg_index_reset[encoded_df_merged_reg_index_reset['Won'] == 1])\n",
    "X_train_lost, X_test_lost, y_train_lost, y_test_lost = prepare_data(encoded_df_merged_reg_index_reset[encoded_df_merged_reg_index_reset['Won'] == 0])\n",
    "\n",
    "# Apply SMOTE to the training data\n",
    "smote_won = SMOTE(random_state=42)\n",
    "X_resampled_won, y_resampled_won = smote_won.fit_resample(X_train_won, y_train_won)\n",
    "\n",
    "# Apply SMOTE to the training data\n",
    "smote_lost = SMOTE(random_state=42)\n",
    "X_resampled_lost, y_resampled_lost = smote_lost.fit_resample(X_train_lost, y_train_lost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_won.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_lost.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Define the hyperparameter search space\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 300)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 5, 30)\n",
    "    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n",
    "    max_features = trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None])\n",
    "    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 4)\n",
    "    \n",
    "    # Create the Random Forest model with the suggested hyperparameters\n",
    "    rf_classifier = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        max_features=max_features,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    rf_classifier.fit(X_resampled_won, y_resampled_won)\n",
    "    \n",
    "    # Make predictions on the validation set\n",
    "    y_pred = rf_classifier.predict(X_test_won)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test_won, y_pred)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Create an Optuna study object and specify the optimization direction\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "\n",
    "# Optimize the study using the objective function\n",
    "study.optimize(objective, n_trials=100, n_jobs=-1)  # Adjust n_trials as needed\n",
    "\n",
    "# Output the best trial\n",
    "print(f\"Best trial:\\n{study.best_trial}\")\n",
    "print(f\"Best accuracy: {study.best_value}\")\n",
    "print(f\"Best hyperparameters: {study.best_trial.params}\")\n",
    "\n",
    "# Extract the best hyperparameters\n",
    "best_params = study.best_trial.params\n",
    "\n",
    "# Train the final model using the best hyperparameters\n",
    "best_rf_classifier = RandomForestClassifier(\n",
    "    n_estimators=best_params[\"n_estimators\"],\n",
    "    max_depth=best_params[\"max_depth\"],\n",
    "    min_samples_split=best_params[\"min_samples_split\"],\n",
    "    max_features=best_params[\"max_features\"],\n",
    "    min_samples_leaf=best_params[\"min_samples_leaf\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model on the full training data\n",
    "best_rf_classifier.fit(X_resampled_won, y_resampled_won)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_best = best_rf_classifier.predict(X_test_won)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Classification Report for 'Won' with Best Hyperparameters:\")\n",
    "print(classification_report(y_test_won, y_pred_best))\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"Confusion Matrix for 'Won' with Best Hyperparameters:\")\n",
    "print(confusion_matrix(y_test_won, y_pred_best))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming y_train_won and y_test_won contain your target labels\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the label encoder and transform the target labels\n",
    "y_resampled_won_encoded = label_encoder.fit_transform(y_resampled_won)\n",
    "y_test_won_encoded = label_encoder.transform(y_test_won)\n",
    "\n",
    "# Print the classes to see the mapping\n",
    "print(\"Classes:\", label_encoder.classes_)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Define the hyperparameter search space\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'use_label_encoder': False,  # Important to avoid warnings in XGBoost > 1.3\n",
    "        'eval_metric': 'mlogloss'  # Metric to use\n",
    "    }\n",
    "\n",
    "    # Create the XGBClassifier model with the suggested hyperparameters\n",
    "    xgb_classifier = XGBClassifier(**param, random_state=42)\n",
    "    \n",
    "    # Train the model\n",
    "    xgb_classifier.fit(X_resampled_won, y_resampled_won_encoded)\n",
    "    \n",
    "    # Make predictions on the validation set\n",
    "    y_pred = xgb_classifier.predict(X_test_won)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test_won_encoded, y_pred)\n",
    "    \n",
    "    return accuracy\n",
    "# Create an Optuna study object and specify the optimization direction\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "\n",
    "# Optimize the study using the objective function\n",
    "study.optimize(objective, n_trials=100, n_jobs=-1)  # Adjust n_trials as needed\n",
    "\n",
    "# Output the best trial\n",
    "print(f\"Best trial:\\n{study.best_trial}\")\n",
    "print(f\"Best accuracy: {study.best_value}\")\n",
    "print(f\"Best hyperparameters: {study.best_trial.params}\")\n",
    "# Extract the best hyperparameters\n",
    "best_params = study.best_trial.params\n",
    "\n",
    "# Train the final model using the best hyperparameters\n",
    "best_xgb_classifier = XGBClassifier(**best_params, random_state=42, use_label_encoder=False, eval_metric='mlogloss')\n",
    "\n",
    "# Fit the model on the full training data\n",
    "best_xgb_classifier.fit(X_resampled_won, y_resampled_won_encoded)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_best = best_xgb_classifier.predict(X_test_won)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Classification Report for 'Won' with Best Hyperparameters:\")\n",
    "print(classification_report(y_test_won_encoded, y_pred_best))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"Confusion Matrix for 'Won' with Best Hyperparameters:\")\n",
    "print(confusion_matrix(y_test_won_encoded, y_pred_best))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best_parameters XGBoost -> Best accuracy: 0.5294117647058824\n",
    "Best hyperparameters: {'n_estimators': 121, 'max_depth': 10, 'learning_rate': 0.2602322469736991, 'subsample': 0.9877578858959679, 'colsample_bytree': 0.9998237546974039, 'gamma': 1.303410777487739, 'reg_alpha': 0.41009343222603883, 'reg_lambda': 0.8400173246665398, 'min_child_weight': 10}\n",
    "\n",
    "best_parameters RF -> Best accuracy: 0.5294117647058824\n",
    "Best hyperparameters: {'n_estimators': 224, 'max_depth': 7, 'min_samples_split': 7, 'max_features': 'sqrt', 'min_samples_leaf': 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base case for comparison against Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_df_base.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a histogram for encoded_df_base['Age']\n",
    "plt.hist(encoded_df_base.loc[encoded_df_base['Won'] == 1, 'Age'], bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. **Survival Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmf = KaplanMeierFitter()\n",
    "kmf.fit(durations=encoded_df_base['Age'], event_observed=encoded_df_base['Won'])\n",
    "kmf.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_df_base.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby Opportunity ID and get the first value from 'Type', 'Age', 'Won', 'Contracted Opportunity ACV' and 'Hospital Count' and find the sum of 'Contracted Weekly Hours',\n",
    "grouped_for_logrank = ofph_index_reset_sel[~ofph_index_reset_sel['Category'].isin([\n",
    "    'Implementation', 'Technology', 'Advisory - Spark', 'Advisory - Validation'])].groupby(['Opportunity ID', 'Category']).agg({\n",
    "    'Type': 'first', 'Age': 'first', 'Won': 'first', 'Contracted Opportunity ACV': 'first', \n",
    "    'Hospital Count': 'first', 'Contracted Weekly Hours': 'sum'#, 'Opportunity ID': 'nunique'\n",
    "    }).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_of_opps_per_cat = grouped_for_logrank.groupby('Category').agg({'Opportunity ID': 'nunique'}).sort_values('Opportunity ID', ascending=False)\n",
    "cats_gr25 = list(count_of_opps_per_cat[count_of_opps_per_cat['Opportunity ID'] > 25].index)\n",
    "cats_gr25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_for_logrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_for_logrank.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = grouped_for_logrank[grouped_for_logrank['Category'] == 'CAT1'].copy()\n",
    "data2 = grouped_for_logrank[grouped_for_logrank['Category'] == 'CAT2'].copy()\n",
    "result = logrank_test(data1['Age'], data2['Age'], \n",
    "                      data1['Won'], data2['Won'])\n",
    "print(result.p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmf = KaplanMeierFitter()\n",
    "kmf.fit(durations=data2['Age'], event_observed=data2['Won'])\n",
    "kmf.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = encoded_df_merged_reg.loc[:, \n",
    "    ((~encoded_df_merged_reg.columns.str.startswith('Category_')) |\n",
    "    (encoded_df_merged_reg.columns.str.replace('Category_', '').isin(cats_gr25)))\n",
    "].copy()\n",
    "\n",
    "# Prepare the target variable\n",
    "df['event_0_30'] = (df['Age'] <= 30).astype(int)\n",
    "df['event_31_60'] = ((df['Age'] > 30) & (df['Age'] <= 60)).astype(int)\n",
    "df['event_60_plus'] = (df['Age'] > 60).astype(int)\n",
    "\n",
    "# Prepare the target variable\n",
    "# df['event'] = pd.cut(df['Age'], bins=[-1, 30, 60, df['Age'].max()], labels=['0-30', '31-60', '60+'])\n",
    "df['duration'] = df['Age']\n",
    "\n",
    "# Prepare features\n",
    "features = ['Category', 'Type', 'Contracted Opportunity ACV', 'Hospital Count', 'Contracted Weekly Hours']\n",
    "\n",
    "# Split the data\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=9999)\n",
    "\n",
    "# Add event columns\n",
    "train_df['event_0_30'] = train_df['event_0_30']\n",
    "train_df['event_31_60'] = train_df['event_31_60']\n",
    "train_df['event_60_plus'] = train_df['event_60_plus']\n",
    "test_df['event_0_30'] = test_df['event_0_30']\n",
    "test_df['event_31_60'] = test_df['event_31_60']\n",
    "test_df['event_60_plus'] = test_df['event_60_plus']\n",
    "\n",
    "# Fit the Cox Proportional Hazards model\n",
    "cph = CoxPHFitter()\n",
    "cph.fit(train_df, duration_col='duration', event_col='event_0_30')\n",
    "\n",
    "# Print the model summary\n",
    "print(cph.print_summary())\n",
    "\n",
    "# Predict survival function for test data\n",
    "test_survival_func_0_30 = cph.predict_survival_function(test_df)\n",
    "\n",
    "# cph2 = CoxPHFitter()\n",
    "# cph2.fit(train_df, duration_col='duration', event_col='event_31_60')\n",
    "# test_survival_func_31_60 = cph2.predict_survival_function(test_df)\n",
    "\n",
    "# cph3 = CoxPHFitter()\n",
    "# cph3.fit(train_df, duration_col='duration', event_col='event_60_plus')\n",
    "# test_survival_func_60_plus = cph3.predict_survival_function(test_df)\n",
    "\n",
    "# Function to predict closure within a given timeframe\n",
    "def predict_closure(survival_func, timeframe):\n",
    "    if timeframe == '0-30':\n",
    "        return 1 - survival_func.loc[30]\n",
    "    elif timeframe == '31-60':\n",
    "        return 1 - survival_func.loc[60]\n",
    "    else:\n",
    "        return 1 - survival_func.loc[test_df['Age'].max()]\n",
    "\n",
    "# Predict closure for 0-30 days, 31-60 days, and beyond 60 days\n",
    "predictions_0_30_days = predict_closure(test_survival_func_0_30, '0-30')\n",
    "# predictions_31_60_days = predict_closure(test_survival_func_31_60, '31-60')\n",
    "# predictions_60_plus_days = predict_closure(test_survival_func_60_plus, '60+')\n",
    "\n",
    "# Add predictions to the test dataframe\n",
    "test_df['prob_close_0_30_days'] = predictions_0_30_days.values\n",
    "# test_df['prob_close_31_60_days'] = predictions_31_60_days.values\n",
    "# test_df['prob_close_60_plus_days'] = predictions_60_plus_days.values\n",
    "\n",
    "# Function to evaluate predictions\n",
    "def evaluate_predictions(df, prob_cols, actual_cols, threshold=0.5):\n",
    "    df['predicted'] = df[prob_cols].idxmax(axis=1)\n",
    "    df['actual_0_30'] = df['event_0_30']\n",
    "    df['actual_31_60'] = df['event_31_60']\n",
    "    df['actual_60_plus'] = df['event_60_plus']\n",
    "    \n",
    "    precision_0_30 = ((df['predicted'] == '0-30') & (df['actual_0_30'] == 1)).sum() / ((df['predicted'] == '0-30')).sum() if ((df['predicted'] == '0-30')).sum() > 0 else 0\n",
    "    recall_0_30 = ((df['predicted'] == '0-30') & (df['actual_0_30'] == 1)).sum() / df['actual_0_30'].sum() if df['actual_0_30'].sum() > 0 else 0\n",
    "    f1_score_0_30 = 2 * (precision_0_30 * recall_0_30) / (precision_0_30 + recall_0_30) if (precision_0_30 + recall_0_30) > 0 else 0\n",
    "\n",
    "    precision_31_60 = ((df['predicted'] == '31-60') & (df['actual_31_60'] == 1)).sum() / ((df['predicted'] == '31-60')).sum() if ((df['predicted'] == '31-60')).sum() > 0 else 0\n",
    "    recall_31_60 = ((df['predicted'] == '31-60') & (df['actual_31_60'] == 1)).sum() / df['actual_31_60'].sum() if df['actual_31_60'].sum() > 0 else 0\n",
    "    f1_score_31_60 = 2 * (precision_31_60 * recall_31_60) / (precision_31_60 + recall_31_60) if (precision_31_60 + recall_31_60) > 0 else 0\n",
    "\n",
    "    precision_60_plus = ((df['predicted'] == '60+') & (df['actual_60_plus'] == 1)).sum() / ((df['predicted'] == '60+')).sum() if ((df['predicted'] == '60+')).sum() > 0 else 0\n",
    "    recall_60_plus = ((df['predicted'] == '60+') & (df['actual_60_plus'] == 1)).sum() / df['actual_60_plus'].sum() if df['actual_60_plus'].sum() > 0 else 0\n",
    "    f1_score_60_plus = 2 * (precision_60_plus * recall_60_plus) / (precision_60_plus + recall_60_plus) if (precision_60_plus + recall_60_plus) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'Precision_0_30': precision_0_30,\n",
    "        'Recall_0_30': recall_0_30, \n",
    "        'F1-Score_0_30': f1_score_0_30,\n",
    "        'Precision_31_60': precision_31_60,\n",
    "        'Recall_31_60': recall_31_60,\n",
    "        'F1-Score_31_60': f1_score_31_60,\n",
    "        'Precision_60_plus': precision_60_plus,\n",
    "        'Recall_60_plus': recall_60_plus,\n",
    "        'F1-Score_60_plus': f1_score_60_plus\n",
    "    }\n",
    "\n",
    "# Evaluate predictions\n",
    "prob_cols = ['prob_close_0_30_days']#, 'prob_close_31_60_days', 'prob_close_60_plus_days']\n",
    "actual_cols = ['event_0_30']#, 'event_31_60', 'event_60_plus']\n",
    "results = evaluate_predictions(test_df, prob_cols, actual_cols)\n",
    "print(\"Prediction results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = encoded_df_merged_reg.loc[:, \n",
    "    ((~encoded_df_merged_reg.columns.str.startswith('Category_')) |\n",
    "    (encoded_df_merged_reg.columns.str.replace('Category_', '').isin(cats_gr25)))\n",
    "].drop(columns='Won').copy()\n",
    "\n",
    "# Prepare the target variable\n",
    "# df['event_0_30'] = (df['Age'] <= 30).astype(int)\n",
    "# df['event_31_60'] = ((df['Age'] > 30) & (df['Age'] <= 60)).astype(int)\n",
    "# df['event_60_plus'] = (df['Age'] > 60).astype(int)\n",
    "\n",
    "# Convert categorical variables to numeric\n",
    "# le = LabelEncoder()\n",
    "# df['Category'] = le.fit_transform(df['Category'])\n",
    "# df['Type'] = le.fit_transform(df['Type'])\n",
    "\n",
    "# Create the target variable (0-30, 31-60, 61+)\n",
    "df['Age_bin'] = pd.cut(df['Age'], bins=[-1, 30, 60, float('inf')], labels=[0, 1, 2])\n",
    "df['Age_bin'] = df['Age_bin'].astype('Int64')\n",
    "# Create the event indicator (1 if the opportunity closed, 0 if it's still open)\n",
    "# df['event'] = 1  # Assuming all opportunities in the dataset have closed\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare the data for the Cox Proportional Hazards model\n",
    "cph_columns = ['Contracted Opportunity ACV', 'Hospital Count', 'Contracted Weekly Hours']\n",
    "\n",
    "# Fit the Cox Proportional Hazards model\n",
    "cph = CoxPHFitter()\n",
    "cph.fit(train_df, duration_col='Age', event_col='event') #, strata=cph_columns\n",
    "\n",
    "# Print the model summary\n",
    "print(cph.print_summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.groupby(['Contracted Opportunity ACV', 'Hospital Count', 'Contracted Weekly Hours']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.drop(columns='Age_bin').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "test_predictions = cph.predict_expectation(test_df)\n",
    "\n",
    "# Convert predictions to age bins\n",
    "test_predictions_bin = pd.cut(test_predictions, bins=[-1, 30, 60, float('inf')], labels=[0, 1, 2])\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (test_predictions_bin == test_df['Age_bin']).mean()\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Calculate precision, recall, and F1-score for each bin\n",
    "\n",
    "print(classification_report(test_df['Age_bin'], test_predictions_bin))\n",
    "\n",
    "# Plot the survival curves for different categories\n",
    "\n",
    "# for category in df['Category'].unique():\n",
    "#     mask = (train_df['Category'] == category)\n",
    "#     cph.plot_partial_effects('Category', values=[category], plot_baseline=False)\n",
    "#     plt.title(f\"Survival Curve for Category {category}\")\n",
    "#     plt.show()\n",
    "\n",
    "# # Feature importance\n",
    "# cph.plot_covariate_groups('Contracted Opportunity ACV', values=[df['Contracted Opportunity ACV'].quantile(0.25), \n",
    "#                                                                 df['Contracted Opportunity ACV'].median(), \n",
    "#                                                                 df['Contracted Opportunity ACV'].quantile(0.75)])\n",
    "# plt.title(\"Impact of Contracted Opportunity ACV on Survival\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(test_df[['Age_bin']], test_predictions_bin.to_frame().rename(columns={0: 'Age_bin_pred'}), left_index=True, right_index=True)\n",
    "merged.loc[:, 'diff'] = merged['Age_bin'] - merged['Age_bin_pred'].astype(int)\n",
    "merged[merged['diff'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features = ['Opportunity ID', ## String\n",
    "                      'Age', # Int\n",
    "                      'Type', # String\n",
    "                      'Category', # String\n",
    "                      'Contracted Weekly Hours', # Float\n",
    "                      'Backlog', # Bool\n",
    "                      'Contracted Opportunity ACV', # Float\n",
    "                      'Hospital Count', # Int\n",
    "                      'Close Date'\n",
    "                    #   'Implemented Weekly Hours', # Float choosing Contracted for classification\n",
    "                      ]\n",
    "cats_gr25 = ['SPECIFIC CATS']\n",
    "ofph_fil = ofph[(ofph['Type'].isin(['New Client', 'Existing Client - New Service Line', 'Existing Client - Service Line Expansion'])) &\n",
    "                 (ofph['Exclude from Resource Requests'] == 'false')].copy()\n",
    "ofph_index_reset = ofph_fil.reset_index()\n",
    "ofph_index_reset_select_cats = ofph_index_reset[ofph_index_reset['Category'].isin(cats_gr25)].copy()\n",
    "# convert the Backlog string column to a boolean by changing the true values to 1 and the false values to 0\n",
    "ofph_index_reset_select_cats['Backlog'] = ofph_index_reset_select_cats['Backlog'].str.replace('true', '1').str.replace('false', '0').astype(int)\n",
    "ofph_index_reset_sel = ofph_index_reset_select_cats[important_features]\n",
    "\n",
    "# groupby Opportunity ID and get the first value from 'Age', 'Backlog', and 'Hospital Count'\n",
    "abhc = ofph_index_reset_sel.groupby(['Opportunity ID', 'Category']).first()[['Age', 'Backlog', 'Hospital Count', 'Contracted Opportunity ACV', 'Close Date']]\n",
    "# group by Opportunity ID and calculate the sum for all the other columns\n",
    "rest = ofph_index_reset_sel.drop(['Age', 'Backlog', 'Hospital Count', 'Contracted Opportunity ACV', 'Close Date'], axis=1).groupby(['Opportunity ID', 'Category']).sum()\n",
    "# merge the 2 back together\n",
    "ofph_index_reset_sel_merged = pd.merge(abhc, rest, left_index=True, right_index=True)\n",
    "ofph_index_reset_sel_merged.loc[:, 'event'] = (ofph_index_reset_sel_merged['Close Date'] <= datetime.now()).astype(int)\n",
    "merged_ready = ofph_index_reset_sel_merged.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KaplanMeierFitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmf = KaplanMeierFitter()\n",
    "# split grouped_for_logrank into train and test\n",
    "train, test = train_test_split(merged_ready, test_size=0.2)\n",
    "# fit the kmf model to the training data\n",
    "kmf.fit(durations=train['Age'], event_observed=train['event'])\n",
    "# predict the survival probability for the test data\n",
    "survival_prob = kmf.predict(test['Age']).to_frame()\n",
    "# add a column to survival_prob that is 1 if the survival probability is greater than 0.75, otherwise 0\n",
    "survival_prob['event'] = survival_prob['KM_estimate'] > 0.95\n",
    "survival_prob['event'] = survival_prob['event'].astype(int)\n",
    "\n",
    "# # calculate the log-rank p-value\n",
    "logrank_test(durations_A=test['Age'], durations_B=survival_prob.index, event_observed_A=test['event'], event_observed_B=survival_prob['event']).p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['event'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index_set = test.reset_index().rename(columns={'event':'actual_event'})\n",
    "survival_prob['row_number'] = range(1, len(survival_prob) + 1)\n",
    "test_index_set['row_number'] = range(1, len(test_index_set) + 1)\n",
    "\n",
    "prob_test_merged = survival_prob.reset_index().merge(test_index_set[['row_number', 'actual_event']], on='row_number', how='inner')\n",
    "prob_test_merged.loc[:, 'diff'] = prob_test_merged['actual_event'] - prob_test_merged['event']\n",
    "print(len(prob_test_merged[prob_test_merged['diff'] != 0])/len(prob_test_merged), len(prob_test_merged[prob_test_merged['diff'] != 0]), len(prob_test_merged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the index of survival_prob on the x-axis and the 'KM_estimate' on the y-axis\n",
    "plt.scatter(survival_prob.index, survival_prob['KM_estimate'])\n",
    "# set the title and axis labels\n",
    "plt.title('Kaplan-Meier Estimate of Survival Probability')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Survival Probability')\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split grouped_for_logrank into train and test\n",
    "train, test = train_test_split(grouped_for_logrank, test_size=0.2, random_state=9999)\n",
    "# fit the kmf model to the training data\n",
    "kmf.fit(durations=train['Age'], event_observed=train['Won'])\n",
    "# predict the survival probability for the test data\n",
    "survival_prob = kmf.predict(test['Age']).to_frame()\n",
    "survival_prob\n",
    "# add a column to survival_prob that is 1 if the survival probability is greater than 0.75, otherwise 0\n",
    "survival_prob['Won'] = survival_prob['KM_estimate'] > 0.75\n",
    "survival_prob['Won'] = survival_prob['Won'].astype(int)\n",
    "survival_prob\n",
    "# # calculate the log-rank p-value\n",
    "logrank_test(durations_A=test['Age'], durations_B=survival_prob.index, event_observed_A=test['Won'], event_observed_B=survival_prob['Won']).p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_for_logrank.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cph = CoxPHFitter()\n",
    "cph.fit(encoded_df_base[['Age','Won', 'Backlog', 'Contracted Weekly Hours', 'Hospital Count']].copy(), duration_col='Age', event_col='Won')\n",
    "cph.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monthly Snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ofph_monthly = ofph[(ofph['Type'].isin(['New Client', 'Existing Client - New Service Line', 'Existing Client - Service Line Expansion'])) &\n",
    "                    (ofph['Exclude from Resource Requests'] == 'false')].copy()\n",
    "ofph_monthly_idx_reset = ofph_monthly.reset_index()\n",
    "# filter out any Technology category\n",
    "ofph_monthly_idx_reset = ofph_monthly_idx_reset[~ofph_monthly_idx_reset['Category'].str.contains('Technology')]\n",
    "# groupby Opportunity ID and get the first value from 'Age', 'Backlog', and 'Hospital Count'\n",
    "abhc = ofph_monthly_idx_reset.groupby(['Opportunity ID', 'Category']).first()[['Opportunity Start Date', 'Close Date', 'Stage']]\n",
    "# group by Opportunity ID and calculate the sum for all the other columns\n",
    "rest = ofph_monthly_idx_reset.groupby(['Opportunity ID', 'Category'])[['Contracted Weekly Hours', 'Implemented Weekly Hours']].sum()\n",
    "# merge the 2 back together\n",
    "ofph_monthly_idx_reset_merged = pd.merge(abhc, rest, left_index=True, right_index=True)\n",
    "ofph_monthly_ready = ofph_monthly_idx_reset_merged.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ofph_monthly_ready['Stage'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_monthly_snapshots(opportunities_df, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Create monthly snapshots of contracted and implemented hours by category.\n",
    "    \n",
    "    Parameters:\n",
    "    opportunities_df: DataFrame with columns ['Category', 'Start_Date', 'End_Date', \n",
    "                     'Contracted_Hours', 'Implemented_Weekly_Hours', 'Stage']\n",
    "    start_date: datetime object for the start of the analysis period\n",
    "    end_date: datetime object for the end of the analysis period\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a date range for all months in the analysis period\n",
    "    months = pd.date_range(start=start_date, end=end_date, freq='MS')\n",
    "    \n",
    "    snapshots = []\n",
    "    \n",
    "    for month_start in months:\n",
    "        month_end = month_start + pd.offsets.MonthEnd(0)\n",
    "        \n",
    "        # Filter opportunities that are active in the current month\n",
    "        active_mask = (\n",
    "            (opportunities_df['Opportunity Start Date'] < month_start) & \n",
    "            (opportunities_df['Close Date'] >= month_end)\n",
    "        )\n",
    "        active_opportunities = opportunities_df[active_mask]\n",
    "        \n",
    "        # Calculate contracted hours by category\n",
    "        contracted_hours = (\n",
    "            active_opportunities\n",
    "            .groupby('Category')['Contracted Weekly Hours']\n",
    "            .sum()\n",
    "            .reset_index()\n",
    "        )\n",
    "        \n",
    "        # Calculate implemented hours only for opportunities with 'Contract Signed (Implementing)' stage\n",
    "        implementing_opportunities = opportunities_df[ \n",
    "            ((opportunities_df['Stage'] == 'Contract Signed (Implementing)') | (opportunities_df['Stage'] == 'Customer Invoiced')) &\n",
    "            (\n",
    "            (opportunities_df['Close Date'] >= month_start) &\n",
    "            (opportunities_df['Close Date'] <= month_end)\n",
    "        )\n",
    "        ].copy()\n",
    "        \n",
    "        implemented_hours = (\n",
    "            implementing_opportunities\n",
    "            .groupby('Category')['Implemented Weekly Hours']\n",
    "            .sum()\n",
    "            .reset_index()\n",
    "        )\n",
    "        \n",
    "        # Merge the results\n",
    "        month_snapshot = pd.merge(\n",
    "            contracted_hours,\n",
    "            implemented_hours,\n",
    "            on='Category',\n",
    "            how='outer'\n",
    "        ).fillna(0)\n",
    "        \n",
    "        # Add month information\n",
    "        month_snapshot['Month'] = month_start.strftime('%Y-%m')\n",
    "        \n",
    "        snapshots.append(month_snapshot)\n",
    "    \n",
    "    # Combine all monthly snapshots\n",
    "    final_df = pd.concat(snapshots, ignore_index=True)\n",
    "    \n",
    "    # Reorder columns\n",
    "    final_df = final_df[['Month', 'Category', 'Contracted Weekly Hours', 'Implemented Weekly Hours']]\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "ofph_snapshots = create_monthly_snapshots(ofph_monthly_ready, '8/1/2022', datetime.now())\n",
    "ofph_snapshots.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ofph_snapshots.loc[:, 'perc_realized'] = ofph_snapshots['Implemented Weekly Hours'] / ofph_snapshots['Contracted Weekly Hours'] \n",
    "ofph_snapshots[ofph_snapshots['Category'] == 'PCI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a double bar graph that shows the Contracted Weekly Hours and Implemented Weekly Hours by Month\n",
    "pci = ofph_snapshots[(ofph_snapshots['Category'] == 'PCI') & (ofph_snapshots['Month'] > '2023-12')]\n",
    "print(pci['perc_realized'].mean(),\n",
    "pci['perc_realized'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = ofph_snapshots.loc[(ofph_snapshots['Month'] > '2023-12'), ['Category', 'Contracted Weekly Hours', 'Implemented Weekly Hours']].groupby('Category').mean()\n",
    "cat.loc[:, 'perc_realized'] = cat['Implemented Weekly Hours'] / cat['Contracted Weekly Hours']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
