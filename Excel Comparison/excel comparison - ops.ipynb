{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path1 = f\"{ops_plan_proj_folder_location}/Todays Ops Plan Data - new env.xlsm\"\n",
    "file_path2 = f\"{ops_plan_proj_folder_location}/Todays Ops Plan Data.xlsm\"\n",
    "new_env_df = pd.read_excel(file_path1, sheet_name=\"SR_Full\")\n",
    "old_env_df = pd.read_excel(file_path2, sheet_name=\"RR_Full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Column names below are arbitrary placeholders to demonstrate data cleaning process\n",
    "# while maintaining privacy of actual business data\n",
    "\n",
    "# drop columns that are not relevant for analysis\n",
    "old_env_cols_dropped = ['Market Segment', 'Performance Score', 'Sub Category', 'Task Priority', 'Request Number',\n",
    "                      'Request Creator', 'Submission Date']\n",
    "new_env_cols_dropped = ['Ticket ID', 'Submitter Name', 'Entry Date', 'Team Reference']\n",
    "\n",
    "# columns to be renamed for consistency between old and new systems\n",
    "old_env_cols_renamed = ['Priority Score', 'Group Name', 'Service Category', 'Support Staff', 'Request Reference',\n",
    "                      'Department Lead', 'Status', 'Service Group', 'Team Lead',\n",
    "                      'Business Unit', 'Active Project', 'OP$`Request Reference`', 'Division']\n",
    "\n",
    "# new names for the columns being renamed\n",
    "old_env_cols_renamed_to = ['Task Priority Total', 'Team Name', 'Primary Service: Type', 'Support Staff: Full Name',\n",
    "                         'Ticket Name', 'Department Manager: Full Name', 'Current Status', 'Service Group Category', 'Team Lead: Full Name',\n",
    "                         'Department Category', 'In Progress', 'OP$`Ticket Name`', 'Business Division']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns that are not needed for comparison\n",
    "old_env_df_dropped = old_env_df.drop(old_env_cols_dropped, axis=1)\n",
    "new_env_df_dropped = new_env_df.drop(new_env_cols_dropped, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what columns are in old that are not in new\n",
    "old_env_cols = old_env_df_dropped.columns\n",
    "new_env_cols = new_env_df.columns\n",
    "old_env_cols_not_in_new = []\n",
    "for col in old_env_cols:\n",
    "    if col not in new_env_cols:\n",
    "        old_env_cols_not_in_new.append(col)\n",
    "print(old_env_cols_not_in_new)\n",
    "# reverse it\n",
    "new_env_cols_not_in_old = []\n",
    "for col in new_env_cols:\n",
    "    if col not in old_env_cols:\n",
    "        new_env_cols_not_in_old.append(col)\n",
    "print(new_env_cols_not_in_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "print(len(old_env_df_dropped.columns))\n",
    "print(len(new_env_df_dropped.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the old_env_df_dropped columns to match the new_env_df_dropped columns for only those that are in old_env_cols_renamed, and rename them to old_env_cols_renamed_to\n",
    "old_env_df_renamed = old_env_df_dropped.rename(columns=dict(zip(old_env_cols_renamed, old_env_cols_renamed_to)))\n",
    "old_env_df_renamed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271\n",
      "264\n"
     ]
    }
   ],
   "source": [
    "# how many unique Ticket names are there in each dataframe? \n",
    "print(len(old_env_df_renamed['Ticket Name'].unique()))\n",
    "print(len(new_env_df_dropped['Ticket Name'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the old_env_df_dropped dataframe to new_env_df_dropped on ['Ticket Request: Ticket Request Id' = 'Source ID: Ticket Request Id'\n",
    "env_joined = pd.merge(old_env_df_renamed, new_env_df_dropped, how='outer', left_on='Ticket Name', \n",
    "                      right_on='Source ID: Request Id')\n",
    "env_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort columns in alphabetical order\n",
    "env_joined = env_joined.reindex(sorted(env_joined.columns), axis=1)\n",
    "# move the 'Source ID: Ticket Request Id' column to the front\n",
    "env_joined = env_joined[['Source ID: Request Id'] + [col for col in env_joined.columns if col != 'Source ID: Request Id']]\n",
    "env_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out the nan values from the 'Source ID:  Request Id' column\n",
    "env_joined_no_nas = env_joined[env_joined['Source ID: Request Id'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_joined_no_nas.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "# for each column, compare the values in the _x column to the values in the _y column and print out the differences in a new dataframe \n",
    "# with the column names, the 'Source ID: Request Id' column and the old and new values\n",
    "env_joined_diffs = pd.DataFrame(columns=['Source ID: Request Id', 'Column Name', 'Old Value', 'New Value'])\n",
    "for col in env_joined_no_nas.columns:\n",
    "    # if column name is not 'Source ID: Request Id' or contain '_y'\n",
    "    if (col != 'Source ID: Request Id') & (col.find('_y') == -1):\n",
    "        for index, row in env_joined_no_nas.iterrows():\n",
    "            # remove the '_x' from the column name\n",
    "            col = col.replace('_x', '')\n",
    "            # if the values in the _x and _y columns are not equal, add the row to the env_joined_diffs dataframe\n",
    "            # however, if they are both strings, and they are 90% similar, don't add them to the dataframe. And if they \n",
    "            # are both NaN, don't add them to the dataframe\n",
    "            if (row[col + '_x'] != row[col + '_y']) & (type(row[col + '_x']) != str) & (type(row[col + '_y']) != str):\n",
    "                env_joined_diffs = pd.concat([env_joined_diffs, pd.DataFrame({'Source ID: Request Id': [row['Source ID: Request Id']],\n",
    "                                                             'Column Name': [col], 'Old Value': [row[col + '_x']], 'New Value': [row[col + '_y']]})])\n",
    "            elif (row[col + '_x'] != row[col + '_y']) & (type(row[col + '_x']) == str) & (type(row[col + '_y']) == str):\n",
    "                # give both column names a similarity score\n",
    "                similarity_score = fuzz.ratio(row[col + '_x'], row[col + '_y'])\n",
    "                # if the similarity score is less than 90, add the row to the env_joined_diffs dataframe\n",
    "                if similarity_score < 85:\n",
    "                    env_joined_diffs = pd.concat([env_joined_diffs, pd.DataFrame({'Source ID: Request Id': [row['Source ID: Request Id']],\n",
    "                                                             'Column Name': [col], 'Old Value': [row[col + '_x']], 'New Value': [row[col + '_y']]})])\n",
    "                    print(row['Source ID:  Request Id'], col, row[col + '_x'], row[col + '_y'], similarity_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out any rows where both the \"Old Value\" and \"New Value\" columns are NaN\n",
    "env_joined_diffs = env_joined_diffs[env_joined_diffs['Old Value'].notna() | env_joined_diffs['New Value'].notna()]\n",
    "\n",
    "env_joined_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send env_joined to excel\n",
    "env_joined_diffs.to_excel(f\"{ops_plan_proj_folder_location}/env_joined_diffs_11_1.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrames are not identical.\n"
     ]
    }
   ],
   "source": [
    "# Rename columns in new_env_df to match old_env_df\n",
    "column_name_mapping = {}\n",
    "for i in range(len(old_env_cols_renamed)):\n",
    "    column_name_mapping[old_env_cols_renamed_to[i]] = old_env_cols_renamed[i]\n",
    "\n",
    "new_env_df_dropped.rename(columns=column_name_mapping, inplace=True)\n",
    "\n",
    "# Set the primary key as the index for both DataFrames\n",
    "old_env_df_dropped.set_index('Request Reference', inplace=True)\n",
    "new_env_df_dropped.set_index('Request Reference', inplace=True)\n",
    "\n",
    "# Compare the two DataFrames\n",
    "are_equal = old_env_df_dropped.equals(new_env_df_dropped)\n",
    "\n",
    "# Check if the DataFrames are equal\n",
    "if are_equal:\n",
    "    print(\"The DataFrames are identical.\")\n",
    "else:\n",
    "    print(\"The DataFrames are not identical.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(old_env_df_dropped.columns)) # 44\n",
    "print(len(new_env_df_dropped.columns)) # 44\n",
    "# how are the two dataframes labeled differently\n",
    "print(old_env_df_dropped.columns.difference(new_env_df_dropped.columns))\n",
    "# compare the index of the two dataframes\n",
    "old_env_df_dropped.index.difference(new_env_df_dropped.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter those index values that are different in the old dataframe\n",
    "old_env_df_dropped[old_env_df_dropped.index.isin(old_env_df_dropped.index.difference(new_env_df_dropped.index))]\n",
    "# filter those index values that are different in the new dataframe\n",
    "new_env_df_dropped[new_env_df_dropped.index.isin(new_env_df_dropped.index.difference(old_env_df_dropped.index))]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
